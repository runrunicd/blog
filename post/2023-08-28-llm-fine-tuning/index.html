<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications | Shel (Run) Zhou&#39;s Blog</title>
<meta name="keywords" content="large-language-model">
<meta name="description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.">
<meta name="author" content="Run Zhou">
<link rel="canonical" href="https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://runrunicd.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://runrunicd.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://runrunicd.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://runrunicd.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://runrunicd.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications" />
<meta property="og:description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-08-28T21:34:17-07:00" />
<meta property="article:modified_time" content="2023-08-28T21:34:17-07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications"/>
<meta name="twitter:description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://runrunicd.github.io/blog/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Build a Simple Framework for LLM Fine-Tuning on TPU \u0026 Applications",
      "item": "https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Build a Simple Framework for LLM Fine-Tuning on TPU \u0026 Applications",
  "name": "Build a Simple Framework for LLM Fine-Tuning on TPU \u0026 Applications",
  "description": "Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.",
  "keywords": [
    "large-language-model"
  ],
  "articleBody": "Introduction I recently delved into two insightful books: ‚ÄúFree Your Mind‚Äù and ‚ÄúThe Inner Game of Tennis‚Äù. Both have shed light on the concept of building systems that encourage growth and resilience. It‚Äôs fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.\nAlthough I had come across these principles in books, it wasn‚Äôt until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.\nWith that in mind, I‚Äôm actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insights, and code in this post.\nGoals Learn the new way of adapting AI to accomplish a binary classification task by building a simple framework of LLM fine-tuning. Learn how to optimize and evaluate AI performance with various conditions or optimization techniques (e.g. training data size). Share insights about the new way vs. the traditional way of adapting AI. How to fine tune an AI Model (LLM) to accomplish a binary classification task? I‚Äôve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I‚Äôve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the OpenLLaMA which is the permissively licensed open source reproduction of Meta AI‚Äôs LLaMA large language model.\nAI models\nGPT-4 (OpenAI): white papaer LLaMA 2 (Meta AI): white paper Claude (Anthropic) LaMDA (Google) PaLM (Google) Gopher (DeepMind) A simple framework Setup: The following resources are helpful in accomplishing the task. Installation \u0026 download: ‚úÖ Set up Google Cloud with GPU/TPU (Note: I have TPU. EasyLM is built for GPU as well)\n‚úÖ Install EasyLM\n‚úÖ Download OpenLLaMA version 3b v2\n‚úÖ Download Rotten Tomatoes data\nCommon installation issues: # Error # https://github.com/huggingface/transformers/issues/19844#issue-1421007669 ImportError: libssl.so.3: cannot open shared object file: No such file or directory # Resolution pip install transformers --force-reinstall # Error sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) # Resolution https://github.com/huggingface/transformers/issues/20011 Step 1: Formulate the target task and tell the model my intention. I want to train a model that can help me predict whether a movie review‚Äôs sentiment is positive or negative.\nFirst of all, I have a generic pre-trained LLM. Here, I chose OpenLLaMA version 3b v2. Secondly, I‚Äôd like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.\nThis is a sample data from output_dataset_train.txt:\n{\"text\": \"[Text]: the rock is destined to be the 21st century's new \\\" conan \\\" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\n[Sentiment]: Positive\"} {\"text\": \"[Text]: the gorgeously elaborate continuation of \\\" the lord of the rings \\\" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\\n[Sentiment]: Positive\"} {\"text\": \"[Text]: interminably bleak , to say nothing of boring .\\n[Sentiment]: Negative\"} {\"text\": \"[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\\n[Sentiment]: Negative\"} The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format. I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.\nThe sample model output on the test data before fine-tuning:\n{\"prefix_text\":[ \"[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\", \"[Text]: consistently clever and suspenseful .\\n[Sentiment]:\",\"[Text]: it's like a \\\" big chill \\\" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\", \"[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\",\"[Text]: red dragon \\\" never cuts corners .\\n[Sentiment]:\", \"[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\"], \"output_text\":[ \"very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\\n[Source]: 1993, Walt Disney Pictures\\n2 comments:\\nOy vey, I've seen this movie at least three times. And I love it. So many songs - so much fun.\\nyup, the kids and i love it, we keep asking for it again...\", \"I knew when I started reading that it was going to end very badly for the hero. I don‚Äôt think I‚Äôve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\\nThis book was provided free of charge for an honest review of this title.\" ...] } The sample model output on the test data after fine-tuning:\n{\"prefix_text\":[ \"[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\", \"[Text]: consistently clever and suspenseful .\\n[Sentiment]:\", \"[Text]: it's like a \\\" big chill \\\" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\", \"[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\", \"[Text]: red dragon \\\" never cuts corners .\\n[Sentiment]:\", \"[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\"], \"output_text\":[\"Positive\",\"Positive\",\"Negative\",\"Positive\",\"Positive\",\"Positive\"],\"temperature\":1.0} Step 2: Fine tune the pre-trained model on the target task labeled training dataset. # Fine tune Tune a pre-trained model # total_steps: number of tokens divided by seq_length=1024 python3 -m EasyLM.models.llama.llama_train \\ --total_steps=1846 \\ --save_model_freq=1846 \\ --optimizer.adamw_optimizer.lr_warmup_steps=184 \\ --train_dataset.json_dataset.path='/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt' \\ --train_dataset.json_dataset.seq_length=1024 \\ --load_checkpoint='params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm' \\ --tokenizer.vocab_file='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model' \\ --logger.output_dir='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned' \\ --mesh_dim='1,4,2' \\ --load_llama_config='3b' \\ --train_dataset.type='json' \\ --train_dataset.text_processor.fields='text' \\ --optimizer.type='adamw' \\ --optimizer.accumulate_gradient_steps=1 \\ --optimizer.adamw_optimizer.lr=0.002 \\ --optimizer.adamw_optimizer.end_lr=0.002 \\ --optimizer.adamw_optimizer.lr_decay_steps=100000000 \\ --optimizer.adamw_optimizer.weight_decay=0.001 \\ --optimizer.adamw_optimizer.multiply_by_parameter_scale=True \\ --optimizer.adamw_optimizer.bf16_momentum=True Step 3: Serve the tuned model. # Serve fine-tuned model # Note: all LlaMA models use the same tonkenizer python3 -m EasyLM.models.llama.llama_serve \\ --load_llama_config='3b' \\ --load_checkpoint='params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params' \\ --tokenizer.vocab_file='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model' Step 4: Evaluate the tuned model on the test dataset. # Evaluate it on the test dataset curl \"http://0.0.0.0:5007/generate\" \\ -H \"Content-Type: application/json\" \\ -X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json | tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json How to improve accuracy ü•≥ Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model‚Äôs performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let‚Äôs experiment with the following variables to find out.\nTraining data size The larger the training data set, the better the model performs. Sampling ratio Training data size Accuracy 50% 4265 50.00% 70% 5971 83.40% 90% 7677 88.74% 100% 8530 87.24% Training data label quality Human subjective judgments are injected into the training dataset, thereby affecting the performance of the model.\nIs the label from the original dataset considered the ground truth? If so, it‚Äôs worth noting that there may be bias involved, as the sentiment of a movie review being categorized as ‚Äòpositive‚Äô or ‚Äônegative‚Äô can be subjective.\nTo understand how the original labels differ from my judgment, I selected 30 movie reviews. I then recalculated the accuracy, using my judgment as the new ground truth, to evaluate the impact of human curation of data labels on model performance. The comparison results are as follows.\nSelected 30 data samples and they reviewed by me:\nText Sentiment (Original) Sentiment (LLM) Sentiment (Mine) lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness . Positive Positive Positive consistently clever and suspenseful . Positive Positive Positive it‚Äôs like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists . Positive Negative Positive the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill . Positive Positive Positive red dragon \" never cuts corners . Positive Positive Positive fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense . Positive Positive Positive throws in enough clever and unexpected twists to make the formula feel fresh . Positive Positive Positive weighty and ponderous but every bit as filling as the treat of the title . Positive Positive Positive a real audience-pleaser that will strike a chord with anyone who‚Äôs ever waited in a doctor‚Äôs office , emergency room , hospital bed or insurance company office . Positive Positive Positive generates an enormous feeling of empathy for its characters . Positive Positive Positive exposing the ways we fool ourselves is one hour photo‚Äôs real strength . Positive Positive Positive it‚Äôs up to you to decide whether to admire these people‚Äôs dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life . Positive Positive Negative mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human . Positive Positive Positive . . . quite good at providing some good old fashioned spooks . Positive Positive Positive at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best . Positive Negative Negative scherfig‚Äôs light-hearted profile of emotional desperation is achingly honest and delightfully cheeky . Positive Positive Positive a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick , and our hearts go out to them as both continue to negotiate their imperfect , love-hate relationship the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh‚Äôs book trainspotting . Positive Negative Negative as it turns out , you can go home again . Positive Negative Negative you‚Äôve already seen city by the sea under a variety of titles , but it‚Äôs worth yet another visit . Positive Positive Positive this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors . Positive Positive Positive making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love‚Äôs power to help people endure almost unimaginable horror . Positive Negative Positive grown-up quibbles are beside the point here . the little girls understand , and mccracken knows that‚Äôs all that matters . Positive Positive Positive a powerful , chilling , and affecting study of one man‚Äôs dying fall . Positive Positive Positive this is a fascinating film because there is no clear-cut hero and no all-out villain . Positive Positive Positive a dreadful day in irish history is given passionate , if somewhat flawed , treatment . Positive Positive Positive . . . a good film that must have baffled the folks in the marketing department . Positive Negative Positive . . . is funny in the way that makes you ache with sadness ( the way chekhov is funny ) , profound without ever being self-important , warm without ever succumbing to sentimentality . Positive Positive Positive devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking . Positive Negative Negative a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds . Positive Positive Positive The accuracy based on the original label vs. my label (sample size = 30):\n# Misclassified Samples Accuracy 8 76.7% 5 83.3% To be continued ‚Ä¶\nHyperparameter tuning How is the new way different from the traditional way of adapting AI? When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.\nThe Traditional Way Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.\nHowever, during my experience in the industry, I‚Äôve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intentions and insights are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.\nThe model-centric approach vs. data-centric approach (Image Source: generated by Midjourney) The New Way In contrast, when adapting a finely-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.\nThe old way vs. new way of AI/ML development (Image Source: Snorkel AI) LLM fine-tuing application is to explore for unsupervised learning‚Ä¶* References ‚Ä¶\nFurther Exploration Exploration of leveraging LLM to predict clusters - unsupervised leanring The impacts of parameters fine-tuning on model performance (e.g. learning rate) Exploration of LoRa in excelerating fine-tuning Thoughts Data collection \u0026 curation I'm reading white paper and human evaluation is now golden standard to assess model performance. I'm thinking for personalized GPT, what would be the best interface to collect personal perference? Application First of all, there's text lol... Can we mine insights and provide data analysis? Can we predict sentiments (BloombergGPT does it) to signal trends? Fine-tuning Temperature: a hyperparameter that controls the randomness of the model's output. When the temperature is close to zero (e.g., 0.1), the model becomes deterministic; when the temperature is high (e.g., 2 or 3), the model's output becomes more random and creative. Okie, according to this information, maybe higher temperature is correlated with higher halluciation? It's worth to try it out. ",
  "wordCount" : "2675",
  "inLanguage": "en",
  "datePublished": "2023-08-28T21:34:17-07:00",
  "dateModified": "2023-08-28T21:34:17-07:00",
  "author":{
    "@type": "Person",
    "name": "Run Zhou"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shel (Run) Zhou's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://runrunicd.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://runrunicd.github.io/blog/" accesskey="h" title="Shel (Run) Zhou&#39;s Blog (Alt + H)">Shel (Run) Zhou&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://runrunicd.github.io/blog/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/%E4%BF%AE/" title="‰øÆ">
                    <span>‰øÆ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://runrunicd.github.io/blog/">Home</a>&nbsp;¬ª&nbsp;<a href="https://runrunicd.github.io/blog/post/">Posts</a></div>
    <h1 class="post-title">
      Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications
    </h1>
    <div class="post-meta"><span title='2023-08-28 21:34:17 -0700 -0700'>August 28, 2023</span>&nbsp;¬∑&nbsp;13 min&nbsp;¬∑&nbsp;Run Zhou

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#goals" aria-label="Goals">Goals</a></li>
                <li>
                    <a href="#how-to-fine-tune-an-ai-model-llm-to-accomplish-a-binary-classification-task" aria-label="How to fine tune an AI Model (LLM) to accomplish a binary classification task?">How to fine tune an AI Model (LLM) to accomplish a binary classification task?</a><ul>
                        
                <li>
                    <a href="#a-simple-framework" aria-label="A simple framework">A simple framework</a><ul>
                        
                <li>
                    <a href="#setup-the-following-resources-are-helpful-in-accomplishing-the-task" aria-label="Setup: The following resources are helpful in accomplishing the task.">Setup: The following resources are helpful in accomplishing the task.</a><ul>
                        
                <li>
                    <a href="#installation--download" aria-label="Installation &amp;amp; download:">Installation &amp; download:</a></li>
                <li>
                    <a href="#common-installation-issues" aria-label="Common installation issues:">Common installation issues:</a></li></ul>
                </li>
                <li>
                    <a href="#step-1-formulate-the-target-task-and-tell-the-model-my-intention" aria-label="Step 1: Formulate the target task and tell the model my intention.">Step 1: Formulate the target task and tell the model my intention.</a></li>
                <li>
                    <a href="#step-2-fine-tune-the-pre-trained-model-on-the-target-task-labeled-training-dataset" aria-label="Step 2: Fine tune the pre-trained model on the target task labeled training dataset.">Step 2: Fine tune the pre-trained model on the target task labeled training dataset.</a></li>
                <li>
                    <a href="#step-3-serve-the-tuned-model" aria-label="Step 3: Serve the tuned model.">Step 3: Serve the tuned model.</a></li>
                <li>
                    <a href="#step-4-evaluate-the-tuned-model-on-the-test-dataset" aria-label="Step 4: Evaluate the tuned model on the test dataset.">Step 4: Evaluate the tuned model on the test dataset.</a></li></ul>
                </li>
                <li>
                    <a href="#how-to-improve-accuracy" aria-label="How to improve accuracy">How to improve accuracy</a><ul>
                        
                <li>
                    <a href="#training-data-size" aria-label="Training data size">Training data size</a></li>
                <li>
                    <a href="#training-data-label-quality" aria-label="Training data label quality">Training data label quality</a></li>
                <li>
                    <a href="#hyperparameter-tuning" aria-label="Hyperparameter tuning">Hyperparameter tuning</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#how-is-the-new-way-different-from-the-traditional-way-of-adapting-ai" aria-label="How is the new way different from the traditional way of adapting AI?">How is the new way different from the traditional way of adapting AI?</a><ul>
                        <ul>
                        
                <li>
                    <a href="#the-traditional-way" aria-label="The Traditional Way">The Traditional Way</a></li>
                <li>
                    <a href="#the-new-way" aria-label="The New Way">The New Way</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a></li>
                <li>
                    <a href="#further-exploration" aria-label="Further Exploration">Further Exploration</a></li>
                <li>
                    <a href="#thoughts" aria-label="Thoughts">Thoughts</a><ul>
                        <ul>
                        
                <li>
                    <a href="#data-collection--curation" aria-label="Data collection &amp;amp; curation">Data collection &amp; curation</a></li>
                <li>
                    <a href="#application" aria-label="Application">Application</a></li>
                <li>
                    <a href="#fine-tuning" aria-label="Fine-tuning">Fine-tuning</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>I recently delved into two insightful books: <a href="https://www.goodreads.com/book/show/121460912-l?from_search=true&amp;from_srp=true&amp;qid=R4FoooIDog&amp;rank=1">&ldquo;Free Your Mind&rdquo;</a> and <a href="https://www.goodreads.com/book/show/905.The_Inner_Game_of_Tennis">&ldquo;The Inner Game of Tennis&rdquo;</a>. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.</p>
<p>Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.</p>
<p>Although I had come across these principles in books, it wasn&rsquo;t until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.</p>
<p>With that in mind, I&rsquo;m actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insights, and code in this post.</p>
<h2 id="goals">Goals<a hidden class="anchor" aria-hidden="true" href="#goals">#</a></h2>
<ul>
<li>Learn the new way of adapting AI to accomplish a binary classification task by building a simple framework of LLM fine-tuning.</li>
<li>Learn how to optimize and evaluate AI performance with various conditions or optimization techniques (e.g. training data size).</li>
<li>Share insights about the new way vs. the traditional way of adapting AI.</li>
</ul>
<h2 id="how-to-fine-tune-an-ai-model-llm-to-accomplish-a-binary-classification-task">How to fine tune an AI Model (LLM) to accomplish a binary classification task?<a hidden class="anchor" aria-hidden="true" href="#how-to-fine-tune-an-ai-model-llm-to-accomplish-a-binary-classification-task">#</a></h2>
<p>I&rsquo;ve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I&rsquo;ve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the <a href="https://github.com/openlm-research/open_llama">OpenLLaMA</a> which is the permissively licensed open source reproduction of Meta AI&rsquo;s <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">LLaMA</a> large language model.</p>
<p><em>AI models</em></p>
<ul>
<li>GPT-4 (OpenAI): <a href="https://arxiv.org/abs/2303.08774">white papaer</a></li>
<li>LLaMA 2 (Meta AI): <a href="https://arxiv.org/abs/2307.09288">white paper</a></li>
<li>Claude (Anthropic)</li>
<li>LaMDA (Google)</li>
<li>PaLM (Google)</li>
<li>Gopher (DeepMind)</li>
</ul>
<h3 id="a-simple-framework">A simple framework<a hidden class="anchor" aria-hidden="true" href="#a-simple-framework">#</a></h3>
<h4 id="setup-the-following-resources-are-helpful-in-accomplishing-the-task">Setup: The following resources are helpful in accomplishing the task.<a hidden class="anchor" aria-hidden="true" href="#setup-the-following-resources-are-helpful-in-accomplishing-the-task">#</a></h4>
<h5 id="installation--download">Installation &amp; download:<a hidden class="anchor" aria-hidden="true" href="#installation--download">#</a></h5>
<p>‚úÖ Set up Google Cloud with GPU/TPU (Note: I have TPU. EasyLM is built for GPU as well)<br>
‚úÖ <a href="https://github.com/young-geng/EasyLM">Install EasyLM</a><br>
‚úÖ <a href="https://huggingface.co/openlm-research/open_llama_3b_v2/tree/main?clone=true">Download OpenLLaMA version 3b v2</a><br>
‚úÖ <a href="https://huggingface.co/datasets/MrbBakh/Rotten_Tomatoes">Download Rotten Tomatoes data</a></p>
<h5 id="common-installation-issues">Common installation issues:<a hidden class="anchor" aria-hidden="true" href="#common-installation-issues">#</a></h5>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Error</span>
</span></span><span class="line"><span class="cl"><span class="c1"># https://github.com/huggingface/transformers/issues/19844#issue-1421007669</span>
</span></span><span class="line"><span class="cl">ImportError: libssl.so.3: cannot open shared object file: No such file or directory
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Resolution</span>
</span></span><span class="line"><span class="cl">pip install transformers --force-reinstall
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Error</span>
</span></span><span class="line"><span class="cl">sentencepiece<span class="se">\s</span>entencepiece<span class="se">\s</span>rc<span class="se">\s</span>entencepiece_processor.cc<span class="o">(</span>1102<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Resolution</span>
</span></span><span class="line"><span class="cl">https://github.com/huggingface/transformers/issues/20011
</span></span></code></pre></div><h4 id="step-1-formulate-the-target-task-and-tell-the-model-my-intention">Step 1: Formulate the target task and tell the model my intention.<a hidden class="anchor" aria-hidden="true" href="#step-1-formulate-the-target-task-and-tell-the-model-my-intention">#</a></h4>
<p>I want to train a model that can help me predict whether a movie review&rsquo;s sentiment is positive or negative.</p>
<p>First of all, I have a generic pre-trained LLM. Here, I chose <a href="https://huggingface.co/openlm-research/open_llama_3b_v2/tree/main?clone=true">OpenLLaMA version 3b v2</a>. Secondly, I&rsquo;d like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.</p>
<p>This is a sample data from <em>output_dataset_train.txt</em>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-txt" data-lang="txt"><span class="line"><span class="cl">{&#34;text&#34;: &#34;[Text]: the rock is destined to be the 21st century&#39;s new \&#34; conan \&#34; and that he&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n[Sentiment]: Positive&#34;}
</span></span><span class="line"><span class="cl">{&#34;text&#34;: &#34;[Text]: the gorgeously elaborate continuation of \&#34; the lord of the rings \&#34; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson&#39;s expanded vision of j . r . r . tolkien&#39;s middle-earth .\n[Sentiment]: Positive&#34;}
</span></span><span class="line"><span class="cl">{&#34;text&#34;: &#34;[Text]: interminably bleak , to say nothing of boring .\n[Sentiment]: Negative&#34;}
</span></span><span class="line"><span class="cl">{&#34;text&#34;: &#34;[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\n[Sentiment]: Negative&#34;}
</span></span></code></pre></div><span style="font-size: 14px; color: grey;">
The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format.
</span>

<p><br></br></p>
<p>I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.</p>
<p>The sample model output on the test data <em>before</em> fine-tuning:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span><span class="nt">&#34;prefix_text&#34;</span><span class="p">:[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: consistently clever and suspenseful .\n[Sentiment]:&#34;</span><span class="p">,</span><span class="s2">&#34;[Text]: it&#39;s like a \&#34; big chill \&#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n[Sentiment]:&#34;</span><span class="p">,</span><span class="s2">&#34;[Text]: red dragon \&#34; never cuts corners .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n[Sentiment]:&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;output_text&#34;</span><span class="p">:[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\n[Source]: 1993, Walt Disney Pictures\n2 comments:\nOy vey, I&#39;ve seen this movie at least three times. And I love it. So many songs - so much fun.\nyup, the kids and i love it, we keep asking for it again...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;I knew when I started reading that it was going to end very badly for the hero. I don‚Äôt think I‚Äôve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\nThis book was provided free of charge for an honest review of this title.&#34;</span> <span class="err">...</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The sample model output on the test data <em>after</em> fine-tuning:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span><span class="nt">&#34;prefix_text&#34;</span><span class="p">:[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: consistently clever and suspenseful .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: it&#39;s like a \&#34; big chill \&#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: red dragon \&#34; never cuts corners .\n[Sentiment]:&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n[Sentiment]:&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;output_text&#34;</span><span class="p">:[</span><span class="s2">&#34;Positive&#34;</span><span class="p">,</span><span class="s2">&#34;Positive&#34;</span><span class="p">,</span><span class="s2">&#34;Negative&#34;</span><span class="p">,</span><span class="s2">&#34;Positive&#34;</span><span class="p">,</span><span class="s2">&#34;Positive&#34;</span><span class="p">,</span><span class="s2">&#34;Positive&#34;</span><span class="p">],</span><span class="nt">&#34;temperature&#34;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">}</span>
</span></span></code></pre></div><h4 id="step-2-fine-tune-the-pre-trained-model-on-the-target-task-labeled-training-dataset">Step 2: Fine tune the pre-trained model on the target task labeled training dataset.<a hidden class="anchor" aria-hidden="true" href="#step-2-fine-tune-the-pre-trained-model-on-the-target-task-labeled-training-dataset">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Fine tune Tune a pre-trained model</span>
</span></span><span class="line"><span class="cl"><span class="c1"># total_steps: number of tokens divided by seq_length=1024</span>
</span></span><span class="line"><span class="cl">python3 -m EasyLM.models.llama.llama_train <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --total_steps<span class="o">=</span><span class="m">1846</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --save_model_freq<span class="o">=</span><span class="m">1846</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.lr_warmup_steps<span class="o">=</span><span class="m">184</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --train_dataset.json_dataset.path<span class="o">=</span><span class="s1">&#39;/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --train_dataset.json_dataset.seq_length<span class="o">=</span><span class="m">1024</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --load_checkpoint<span class="o">=</span><span class="s1">&#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --tokenizer.vocab_file<span class="o">=</span><span class="s1">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --logger.output_dir<span class="o">=</span><span class="s1">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned&#39;</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --mesh_dim<span class="o">=</span><span class="s1">&#39;1,4,2&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --load_llama_config<span class="o">=</span><span class="s1">&#39;3b&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --train_dataset.type<span class="o">=</span><span class="s1">&#39;json&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --train_dataset.text_processor.fields<span class="o">=</span><span class="s1">&#39;text&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.type<span class="o">=</span><span class="s1">&#39;adamw&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.accumulate_gradient_steps<span class="o">=</span><span class="m">1</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.lr<span class="o">=</span>0.002 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.end_lr<span class="o">=</span>0.002 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.lr_decay_steps<span class="o">=</span><span class="m">100000000</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.weight_decay<span class="o">=</span>0.001 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.multiply_by_parameter_scale<span class="o">=</span>True <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --optimizer.adamw_optimizer.bf16_momentum<span class="o">=</span>True 
</span></span></code></pre></div><h4 id="step-3-serve-the-tuned-model">Step 3: Serve the tuned model.<a hidden class="anchor" aria-hidden="true" href="#step-3-serve-the-tuned-model">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Serve fine-tuned model</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Note: all LlaMA models use the same tonkenizer </span>
</span></span><span class="line"><span class="cl">python3 -m EasyLM.models.llama.llama_serve <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --load_llama_config<span class="o">=</span><span class="s1">&#39;3b&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --load_checkpoint<span class="o">=</span><span class="s1">&#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --tokenizer.vocab_file<span class="o">=</span><span class="s1">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model&#39;</span>
</span></span></code></pre></div><h4 id="step-4-evaluate-the-tuned-model-on-the-test-dataset">Step 4: Evaluate the tuned model on the test dataset.<a hidden class="anchor" aria-hidden="true" href="#step-4-evaluate-the-tuned-model-on-the-test-dataset">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Evaluate it on the test dataset</span>
</span></span><span class="line"><span class="cl">curl <span class="s2">&#34;http://0.0.0.0:5007/generate&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-H <span class="s2">&#34;Content-Type: application/json&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json <span class="p">|</span> tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json
</span></span></code></pre></div><p><br></br></p>
<h3 id="how-to-improve-accuracy">How to improve accuracy<a hidden class="anchor" aria-hidden="true" href="#how-to-improve-accuracy">#</a></h3>
<p>ü•≥ Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model&rsquo;s performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let&rsquo;s experiment with the following variables to find out.</p>
<h4 id="training-data-size">Training data size<a hidden class="anchor" aria-hidden="true" href="#training-data-size">#</a></h4>
<p><span style="background-color: #FFDAB9"> The larger the training data set, the better the model performs. </span></p>
<table>
<thead>
<tr>
<th>Sampling ratio</th>
<th>Training data size</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>50%</td>
<td>4265</td>
<td>50.00%</td>
</tr>
<tr>
<td>70%</td>
<td>5971</td>
<td>83.40%</td>
</tr>
<tr>
<td>90%</td>
<td>7677</td>
<td>88.74%</td>
</tr>
<tr>
<td>100%</td>
<td>8530</td>
<td>87.24%</td>
</tr>
</tbody>
</table>
<h4 id="training-data-label-quality">Training data label quality<a hidden class="anchor" aria-hidden="true" href="#training-data-label-quality">#</a></h4>
<p><span style="background-color: #FFDAB9">Human subjective judgments are injected into the training dataset, thereby affecting the performance of the model.</span></p>
<p>Is the label from the original dataset considered the ground truth? If so, it&rsquo;s worth noting that there may be bias involved, as the sentiment of a movie review being categorized as &lsquo;positive&rsquo; or &rsquo;negative&rsquo; can be subjective.</p>
<p>To understand how the original labels differ from my judgment, I selected 30 movie reviews. I then recalculated the accuracy, using my judgment as the new ground truth, to evaluate the impact of human curation of data labels on model performance. The comparison results are as follows.</p>
<p><em>Selected 30 data samples and they reviewed by me:</em></p>
<div style="width: 100%; max-height: 400px; overflow: auto; border: 1px solid #ccc; font-size: 12px;;">
<table>
<thead>
<tr>
<th>Text</th>
<th>Sentiment (Original)</th>
<th>Sentiment (LLM)</th>
<th>Sentiment (Mine)</th>
</tr>
</thead>
<tbody>
<tr>
<td>lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>consistently clever and suspenseful .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>it&rsquo;s like a &quot; big chill &quot; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .</td>
<td>Positive</td>
<td>Negative</td>
<td>Positive</td>
</tr>
<tr>
<td>the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>red dragon &quot; never cuts corners .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>throws in enough clever and unexpected twists to make the formula feel fresh .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>weighty and ponderous but every bit as filling as the treat of the title .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>a real audience-pleaser that will strike a chord with anyone who&rsquo;s ever waited in a doctor&rsquo;s office , emergency room , hospital bed or insurance company office .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>generates an enormous feeling of empathy for its characters .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>exposing the ways we fool ourselves is one hour photo&rsquo;s real strength .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>it&rsquo;s up to you to decide whether to admire these people&rsquo;s dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life .</td>
<td>Positive</td>
<td>Positive</td>
<td>Negative</td>
</tr>
<tr>
<td>mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>. . . quite good at providing some good old fashioned spooks .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best .</td>
<td>Positive</td>
<td>Negative</td>
<td>Negative</td>
</tr>
<tr>
<td>scherfig&rsquo;s light-hearted profile of emotional desperation is achingly honest and delightfully cheeky .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick , and our hearts go out to them as both continue to negotiate their imperfect , love-hate relationship</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh&rsquo;s book trainspotting .</td>
<td>Positive</td>
<td>Negative</td>
<td>Negative</td>
</tr>
<tr>
<td>as it turns out , you can go home again .</td>
<td>Positive</td>
<td>Negative</td>
<td>Negative</td>
</tr>
<tr>
<td>you&rsquo;ve already seen city by the sea under a variety of titles , but it&rsquo;s worth yet another visit .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love&rsquo;s power to help people endure almost unimaginable horror .</td>
<td>Positive</td>
<td>Negative</td>
<td>Positive</td>
</tr>
<tr>
<td>grown-up quibbles are beside the point here . the little girls understand , and mccracken knows that&rsquo;s all that matters .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>a powerful , chilling , and affecting study of one man&rsquo;s dying fall .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>this is a fascinating film because there is no clear-cut hero and no all-out villain .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>a dreadful day in irish history is given passionate , if somewhat flawed , treatment .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>. . . a good film that must have baffled the folks in the marketing department .</td>
<td>Positive</td>
<td>Negative</td>
<td>Positive</td>
</tr>
<tr>
<td>. . . is funny in the way that makes you ache with sadness ( the way chekhov is funny ) , profound without ever being self-important , warm without ever succumbing to sentimentality .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
<tr>
<td>devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking .</td>
<td>Positive</td>
<td>Negative</td>
<td>Negative</td>
</tr>
<tr>
<td>a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds .</td>
<td>Positive</td>
<td>Positive</td>
<td>Positive</td>
</tr>
</tbody>
</table>
</div>  
<p><br></br></p>
<p><em>The accuracy based on the original label vs. my label (sample size = 30):</em></p>
<table>
<thead>
<tr>
<th># Misclassified Samples</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>76.7%</td>
</tr>
<tr>
<td>5</td>
<td>83.3%</td>
</tr>
</tbody>
</table>
<p><em>To be continued &hellip;</em></p>
<h4 id="hyperparameter-tuning">Hyperparameter tuning<a hidden class="anchor" aria-hidden="true" href="#hyperparameter-tuning">#</a></h4>
<h2 id="how-is-the-new-way-different-from-the-traditional-way-of-adapting-ai">How is the new way different from the traditional way of adapting AI?<a hidden class="anchor" aria-hidden="true" href="#how-is-the-new-way-different-from-the-traditional-way-of-adapting-ai">#</a></h2>
<p>When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.</p>
<h4 id="the-traditional-way">The Traditional Way<a hidden class="anchor" aria-hidden="true" href="#the-traditional-way">#</a></h4>
<p>Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.</p>
<p>However, during my experience in the industry, I&rsquo;ve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intentions and insights are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.</p>
<p><br></br></p>
<p><img loading="lazy" src="images/model_vs_data_centric.png#center" alt="Model-Centric vs Data-Centric"  />
</p>
<div style="font-size: 14px; color: grey; text-align: center;">
  
The model-centric approach vs. data-centric approach (Image Source: generated by Midjourney)

</div>


<h4 id="the-new-way">The New Way<a hidden class="anchor" aria-hidden="true" href="#the-new-way">#</a></h4>
<p>In contrast, when adapting a finely-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.</p>
<p><img loading="lazy" src="images/new_vs_old_ai_dev.png#center" alt="old_way"  />

<div style="font-size: 14px; color: grey; text-align: center;">
  
The old way vs. new way of AI/ML development (Image Source: Snorkel AI)

</div>

</p>
<ul>
<li>LLM fine-tuing application is to explore for unsupervised learning&hellip;*</li>
</ul>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>&hellip;</p>
<h2 id="further-exploration">Further Exploration<a hidden class="anchor" aria-hidden="true" href="#further-exploration">#</a></h2>
<ul>
<li>Exploration of leveraging LLM to predict clusters - unsupervised leanring</li>
<li>The impacts of parameters fine-tuning on model performance (e.g. learning rate)</li>
<li>Exploration of LoRa in excelerating fine-tuning</li>
</ul>
<h2 id="thoughts">Thoughts<a hidden class="anchor" aria-hidden="true" href="#thoughts">#</a></h2>
<h4 id="data-collection--curation">Data collection &amp; curation<a hidden class="anchor" aria-hidden="true" href="#data-collection--curation">#</a></h4>
<pre><code> I'm reading white paper and human evaluation is now golden standard to assess model performance. I'm thinking for personalized GPT, what would be the best interface to collect personal perference?
</code></pre>
<h4 id="application">Application<a hidden class="anchor" aria-hidden="true" href="#application">#</a></h4>
<pre><code> First of all, there's text lol... Can we mine insights and provide data analysis? Can we predict sentiments (BloombergGPT does it) to signal trends? 
</code></pre>
<h4 id="fine-tuning">Fine-tuning<a hidden class="anchor" aria-hidden="true" href="#fine-tuning">#</a></h4>
<pre><code> Temperature: a hyperparameter that controls the randomness of the model's output. When the temperature is close to zero (e.g., 0.1), the model becomes deterministic; when the temperature is high (e.g., 2 or 3), the model's output becomes more random and creative. Okie, according to this information, maybe higher temperature is correlated with higher halluciation? It's worth to try it out.
</code></pre>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://runrunicd.github.io/blog/tags/large-language-model/">large-language-model</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://runrunicd.github.io/blog/post/2023-07-27-inner-game/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>The Inner Game</span>
  </a>
  <a class="next" href="https://runrunicd.github.io/blog/post/2023-08-28-llm-reading-notes/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Learning Notes on Large Language Model (Dumping Knowledge for Now)</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on twitter"
        href="https://twitter.com/intent/tweet/?text=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications&amp;url=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f&amp;hashtags=large-language-model">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f&amp;title=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications&amp;summary=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications&amp;source=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f&title=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on whatsapp"
        href="https://api.whatsapp.com/send?text=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications%20-%20https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on telegram"
        href="https://telegram.me/share/url?text=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications&amp;url=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Applications on ycombinator"
        href="https://news.ycombinator.com/submitlink?t=Build%20a%20Simple%20Framework%20for%20LLM%20Fine-Tuning%20on%20TPU%20%26%20Applications&u=https%3a%2f%2frunrunicd.github.io%2fblog%2fpost%2f2023-08-28-llm-fine-tuning%2f">
        <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
            xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
            <path
                d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://runrunicd.github.io/blog/">Shel (Run) Zhou&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
