<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>A Simple Framework for LLM Fine-Tuning on TPU &amp; Some Insights | Shel (Run) Zhou&#39;s Blog</title>
<meta name="keywords" content="large-language-model">
<meta name="description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.">
<meta name="author" content="">
<link rel="canonical" href="https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://runrunicd.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://runrunicd.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://runrunicd.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://runrunicd.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://runrunicd.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="A Simple Framework for LLM Fine-Tuning on TPU &amp; Some Insights" />
<meta property="og:description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-08-28T21:34:17-07:00" />
<meta property="article:modified_time" content="2023-08-28T21:34:17-07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A Simple Framework for LLM Fine-Tuning on TPU &amp; Some Insights"/>
<meta name="twitter:description" content="Introduction I recently delved into two insightful books: &ldquo;Free Your Mind&rdquo; and &ldquo;The Inner Game of Tennis&rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://runrunicd.github.io/blog/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "A Simple Framework for LLM Fine-Tuning on TPU \u0026 Some Insights",
      "item": "https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "A Simple Framework for LLM Fine-Tuning on TPU \u0026 Some Insights",
  "name": "A Simple Framework for LLM Fine-Tuning on TPU \u0026 Some Insights",
  "description": "Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.",
  "keywords": [
    "large-language-model"
  ],
  "articleBody": "Introduction I recently delved into two insightful books: “Free Your Mind” and “The Inner Game of Tennis”. Both have shed light on the concept of building systems that encourage growth and resilience. It’s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.\nAlthough I had come across these principles in books, it wasn’t until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.\nWith that in mind, I’m actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insights, and code in this post.\nGoals Learn the new way of adapting AI to accomplish a binary classification task by building a simple framework of LLM fine-tuning. Learn how to optimize and evaluate AI performance with various conditions or optimization techniques (e.g. training data size). Share insights about the new way vs. the traditional way of adapting AI. How to fine tune a LLM to accomplish a binary classification task? I’ve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I’ve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the OpenLLaMA which is the permissively licensed open source reproduction of Meta AI’s LLaMA large language model.\nA simple framework Setup: The following resources are helpful in accomplishing the task. Installation \u0026 Download: ✅ Set up Google Cloud with GPU/TPU (Note: I have TPU. EasyLM is built for GPU as well)\n✅ Install EasyLM\n✅ Download OpenLLaMA version 3b 2v\n✅ Download Rotten Tomatoes data\nCommon Installation Issues: # Error # https://github.com/huggingface/transformers/issues/19844#issue-1421007669 ImportError: libssl.so.3: cannot open shared object file: No such file or directory # Resolution pip install transformers --force-reinstall # Error sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) # Resolution https://github.com/huggingface/transformers/issues/20011 Step 1: Formulate the target task and tell the model my intention. I want to train a model that can help me predict whether a movie review’s sentiment is positive or negative.\nFirst of all, I have a generic pre-trained LLM. Here, I chose OpenLLaMA version 3b 2v. Secondly, I’d like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.\nThis is a sample data from output_dataset_train.txt:\n{\"text\": \"[Text]: the rock is destined to be the 21st century's new \\\" conan \\\" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\n[Sentiment]: Positive\"} {\"text\": \"[Text]: the gorgeously elaborate continuation of \\\" the lord of the rings \\\" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\\n[Sentiment]: Positive\"} {\"text\": \"[Text]: interminably bleak , to say nothing of boring .\\n[Sentiment]: Negative\"} {\"text\": \"[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\\n[Sentiment]: Negative\"} The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format. I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.\nThe sample model output on the test data before fine-tuning:\n{\"prefix_text\":[ \"[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\", \"[Text]: consistently clever and suspenseful .\\n[Sentiment]:\",\"[Text]: it's like a \\\" big chill \\\" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\", \"[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\",\"[Text]: red dragon \\\" never cuts corners .\\n[Sentiment]:\", \"[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\"], \"output_text\":[ \"very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\\n[Source]: 1993, Walt Disney Pictures\\n2 comments:\\nOy vey, I've seen this movie at least three times. And I love it. So many songs - so much fun.\\nyup, the kids and i love it, we keep asking for it again...\", \"I knew when I started reading that it was going to end very badly for the hero. I don’t think I’ve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\\nThis book was provided free of charge for an honest review of this title.\" ...] } The sample model output on the test data after fine-tuning:\n{\"prefix_text\":[ \"[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\", \"[Text]: consistently clever and suspenseful .\\n[Sentiment]:\", \"[Text]: it's like a \\\" big chill \\\" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\", \"[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\", \"[Text]: red dragon \\\" never cuts corners .\\n[Sentiment]:\", \"[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\"], \"output_text\":[\"Positive\",\"Positive\",\"Negative\",\"Positive\",\"Positive\",\"Positive\"],\"temperature\":1.0} Step 2: Fine tune the pre-trained model (OpenLLaMA 3b v2) on the target task labeled training dataset. # Fine tune Tune a pre-trained model # total_steps: number of tokens divided by seq_length=1024 python3 -m EasyLM.models.llama.llama_train \\ --total_steps=1846 \\ --save_model_freq=1846 \\ --optimizer.adamw_optimizer.lr_warmup_steps=184 \\ --train_dataset.json_dataset.path='/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt' \\ --train_dataset.json_dataset.seq_length=1024 \\ --load_checkpoint='params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm' \\ --tokenizer.vocab_file='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model' \\ --logger.output_dir='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned' \\ --mesh_dim='1,4,2' \\ --load_llama_config='3b' \\ --train_dataset.type='json' \\ --train_dataset.text_processor.fields='text' \\ --optimizer.type='adamw' \\ --optimizer.accumulate_gradient_steps=1 \\ --optimizer.adamw_optimizer.lr=0.002 \\ --optimizer.adamw_optimizer.end_lr=0.002 \\ --optimizer.adamw_optimizer.lr_decay_steps=100000000 \\ --optimizer.adamw_optimizer.weight_decay=0.001 \\ --optimizer.adamw_optimizer.multiply_by_parameter_scale=True \\ --optimizer.adamw_optimizer.bf16_momentum=True Step 3: Serve the tuned model. # Serve fine-tuned model # Note: all LlaMA models use the same tonkenizer python3 -m EasyLM.models.llama.llama_serve \\ --load_llama_config='3b' \\ --load_checkpoint='params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params' \\ --tokenizer.vocab_file='/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model' Step 4: Evaluate the tuned model on the test dataset. # Evaluate it on the test dataset curl \"http://0.0.0.0:5007/generate\" \\ -H \"Content-Type: application/json\" \\ -X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json | tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json Evaluation 🥳 Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model’s performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let’s experiment with the following variables to find out.\nTraining data size As training data increases, the model performs better on the same test dataset (size = X rows).\nSampling ratio Training data size Accuracy 50% 4265 50.00% 70% 5971 83.40% 90% 7677 88.74% 100% 8530 87.24% To be continued …\nHow is the new way different from the traditional way of adapting AI? When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.\nThe Traditional Way Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.\nHowever, during my experience in the industry, I’ve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intentions and insights are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.\nThe model-centric approach vs. data-centric approach (Image Source: generated by Midjourney) The New Way In contrast, when adapting a finely-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.\nThe old way vs. new way of AI/ML development (Image Source: Snorkel AI) Parameter tuning is to be experimented…\nUnsupervised learning is to be explored using LLM fine-tuing…\nReferences [1]\nNext Steps Exploration of leveraging LLM to predict clusters Investigate misclassified samples to gather insights about data collection \u0026 quality The impacts of parameters fine-tuning on model performance (e.g. learning rate) Exploration of LoRa in excelerating fine-tuning ",
  "wordCount" : "1683",
  "inLanguage": "en",
  "datePublished": "2023-08-28T21:34:17-07:00",
  "dateModified": "2023-08-28T21:34:17-07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shel (Run) Zhou's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://runrunicd.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://runrunicd.github.io/blog/" accesskey="h" title="Shel (Run) Zhou&#39;s Blog (Alt + H)">Shel (Run) Zhou&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://runrunicd.github.io/blog/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://runrunicd.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      A Simple Framework for LLM Fine-Tuning on TPU &amp; Some Insights
    </h1>
    <div class="post-meta"><span title='2023-08-28 21:34:17 -0700 -0700'>August 28, 2023</span>

</div>
  </header> 
  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>I recently delved into two insightful books: <a href="https://www.goodreads.com/book/show/121460912-l?from_search=true&amp;from_srp=true&amp;qid=R4FoooIDog&amp;rank=1">&ldquo;Free Your Mind&rdquo;</a> and <a href="https://www.goodreads.com/book/show/905.The_Inner_Game_of_Tennis">&ldquo;The Inner Game of Tennis&rdquo;</a>. Both have shed light on the concept of building systems that encourage growth and resilience. It&rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.</p>
<p>Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.</p>
<p>Although I had come across these principles in books, it wasn&rsquo;t until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.</p>
<p>With that in mind, I&rsquo;m actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insights, and code in this post.</p>
<h2 id="goals">Goals<a hidden class="anchor" aria-hidden="true" href="#goals">#</a></h2>
<ul>
<li>Learn the new way of adapting AI to accomplish a binary classification task by building a simple framework of LLM fine-tuning.</li>
<li>Learn how to optimize and evaluate AI performance with various conditions or optimization techniques (e.g. training data size).</li>
<li>Share insights about the new way vs. the traditional way of adapting AI.</li>
</ul>
<h2 id="how-to-fine-tune-a-llm-to-accomplish-a-binary-classification-task">How to fine tune a LLM to accomplish a binary classification task?<a hidden class="anchor" aria-hidden="true" href="#how-to-fine-tune-a-llm-to-accomplish-a-binary-classification-task">#</a></h2>
<p>I&rsquo;ve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I&rsquo;ve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the <a href="https://github.com/openlm-research/open_llama">OpenLLaMA</a> which is the permissively licensed open source reproduction of Meta AI&rsquo;s <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">LLaMA</a> large language model.</p>
<h3 id="a-simple-framework">A simple framework<a hidden class="anchor" aria-hidden="true" href="#a-simple-framework">#</a></h3>
<h4 id="setup-the-following-resources-are-helpful-in-accomplishing-the-task">Setup: The following resources are helpful in accomplishing the task.<a hidden class="anchor" aria-hidden="true" href="#setup-the-following-resources-are-helpful-in-accomplishing-the-task">#</a></h4>
<h5 id="installation--download">Installation &amp; Download:<a hidden class="anchor" aria-hidden="true" href="#installation--download">#</a></h5>
<p>✅ Set up Google Cloud with GPU/TPU (Note: I have TPU. EasyLM is built for GPU as well)<br>
✅ <a href="https://github.com/young-geng/EasyLM">Install EasyLM</a><br>
✅ <a href="https://huggingface.co/openlm-research/open_llama_3b_v2/tree/main?clone=true">Download OpenLLaMA version 3b 2v</a><br>
✅ <a href="https://huggingface.co/datasets/MrbBakh/Rotten_Tomatoes">Download Rotten Tomatoes data</a></p>
<h5 id="common-installation-issues">Common Installation Issues:<a hidden class="anchor" aria-hidden="true" href="#common-installation-issues">#</a></h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Error</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># https://github.com/huggingface/transformers/issues/19844#issue-1421007669</span>
</span></span><span style="display:flex;"><span>ImportError: libssl.so.3: cannot open shared object file: No such file or directory
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Resolution</span>
</span></span><span style="display:flex;"><span>pip install transformers --force-reinstall
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Error</span>
</span></span><span style="display:flex;"><span>sentencepiece<span style="color:#ae81ff">\s</span>entencepiece<span style="color:#ae81ff">\s</span>rc<span style="color:#ae81ff">\s</span>entencepiece_processor.cc<span style="color:#f92672">(</span>1102<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Resolution</span>
</span></span><span style="display:flex;"><span>https://github.com/huggingface/transformers/issues/20011
</span></span></code></pre></div><h4 id="step-1-formulate-the-target-task-and-tell-the-model-my-intention">Step 1: Formulate the target task and tell the model my intention.<a hidden class="anchor" aria-hidden="true" href="#step-1-formulate-the-target-task-and-tell-the-model-my-intention">#</a></h4>
<p>I want to train a model that can help me predict whether a movie review&rsquo;s sentiment is positive or negative.</p>
<p>First of all, I have a generic pre-trained LLM. Here, I chose <a href="https://huggingface.co/openlm-research/open_llama_3b_v2/tree/main?clone=true">OpenLLaMA version 3b 2v</a>. Secondly, I&rsquo;d like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.</p>
<p>This is a sample data from <em>output_dataset_train.txt</em>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>{&#34;text&#34;: &#34;[Text]: the rock is destined to be the 21st century&#39;s new \&#34; conan \&#34; and that he&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n[Sentiment]: Positive&#34;}
</span></span><span style="display:flex;"><span>{&#34;text&#34;: &#34;[Text]: the gorgeously elaborate continuation of \&#34; the lord of the rings \&#34; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson&#39;s expanded vision of j . r . r . tolkien&#39;s middle-earth .\n[Sentiment]: Positive&#34;}
</span></span><span style="display:flex;"><span>{&#34;text&#34;: &#34;[Text]: interminably bleak , to say nothing of boring .\n[Sentiment]: Negative&#34;}
</span></span><span style="display:flex;"><span>{&#34;text&#34;: &#34;[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\n[Sentiment]: Negative&#34;}
</span></span></code></pre></div><span style="font-size: 14px; color: grey;">
The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format.
</span>

<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.</p>
<p>The sample model output on the test data <em>before</em> fine-tuning:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{<span style="color:#f92672">&#34;prefix_text&#34;</span>:[
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: consistently clever and suspenseful .\n[Sentiment]:&#34;</span>,<span style="color:#e6db74">&#34;[Text]: it&#39;s like a \&#34; big chill \&#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n[Sentiment]:&#34;</span>,<span style="color:#e6db74">&#34;[Text]: red dragon \&#34; never cuts corners .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n[Sentiment]:&#34;</span>],
</span></span><span style="display:flex;"><span><span style="color:#f92672">&#34;output_text&#34;</span>:[
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\n[Source]: 1993, Walt Disney Pictures\n2 comments:\nOy vey, I&#39;ve seen this movie at least three times. And I love it. So many songs - so much fun.\nyup, the kids and i love it, we keep asking for it again...&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;I knew when I started reading that it was going to end very badly for the hero. I don’t think I’ve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\nThis book was provided free of charge for an honest review of this title.&#34;</span> <span style="color:#960050;background-color:#1e0010">...</span>]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The sample model output on the test data <em>after</em> fine-tuning:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{<span style="color:#f92672">&#34;prefix_text&#34;</span>:[
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: consistently clever and suspenseful .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: it&#39;s like a \&#34; big chill \&#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: red dragon \&#34; never cuts corners .\n[Sentiment]:&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n[Sentiment]:&#34;</span>],
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;output_text&#34;</span>:[<span style="color:#e6db74">&#34;Positive&#34;</span>,<span style="color:#e6db74">&#34;Positive&#34;</span>,<span style="color:#e6db74">&#34;Negative&#34;</span>,<span style="color:#e6db74">&#34;Positive&#34;</span>,<span style="color:#e6db74">&#34;Positive&#34;</span>,<span style="color:#e6db74">&#34;Positive&#34;</span>],<span style="color:#f92672">&#34;temperature&#34;</span>:<span style="color:#ae81ff">1.0</span>}
</span></span></code></pre></div><h4 id="step-2-fine-tune-the-pre-trained-model-openllama-3b-v2-on-the-target-task-labeled-training-dataset">Step 2: Fine tune the pre-trained model (OpenLLaMA 3b v2) on the target task labeled training dataset.<a hidden class="anchor" aria-hidden="true" href="#step-2-fine-tune-the-pre-trained-model-openllama-3b-v2-on-the-target-task-labeled-training-dataset">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Fine tune Tune a pre-trained model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># total_steps: number of tokens divided by seq_length=1024</span>
</span></span><span style="display:flex;"><span>python3 -m EasyLM.models.llama.llama_train <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --total_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1846</span>  <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --save_model_freq<span style="color:#f92672">=</span><span style="color:#ae81ff">1846</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.lr_warmup_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">184</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --train_dataset.json_dataset.path<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --train_dataset.json_dataset.seq_length<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --load_checkpoint<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --tokenizer.vocab_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --logger.output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned&#39;</span>  <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --mesh_dim<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1,4,2&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --load_llama_config<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;3b&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --train_dataset.type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;json&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --train_dataset.text_processor.fields<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;text&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adamw&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.accumulate_gradient_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.lr<span style="color:#f92672">=</span>0.002 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.end_lr<span style="color:#f92672">=</span>0.002 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.lr_decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">100000000</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.weight_decay<span style="color:#f92672">=</span>0.001 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.multiply_by_parameter_scale<span style="color:#f92672">=</span>True <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --optimizer.adamw_optimizer.bf16_momentum<span style="color:#f92672">=</span>True 
</span></span></code></pre></div><h4 id="step-3-serve-the-tuned-model">Step 3: Serve the tuned model.<a hidden class="anchor" aria-hidden="true" href="#step-3-serve-the-tuned-model">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Serve fine-tuned model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note: all LlaMA models use the same tonkenizer </span>
</span></span><span style="display:flex;"><span>python3 -m EasyLM.models.llama.llama_serve <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --load_llama_config<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;3b&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --load_checkpoint<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --tokenizer.vocab_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model&#39;</span>
</span></span></code></pre></div><h4 id="step-4-evaluate-the-tuned-model-on-the-test-dataset">Step 4: Evaluate the tuned model on the test dataset.<a hidden class="anchor" aria-hidden="true" href="#step-4-evaluate-the-tuned-model-on-the-test-dataset">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Evaluate it on the test dataset</span>
</span></span><span style="display:flex;"><span>curl <span style="color:#e6db74">&#34;http://0.0.0.0:5007/generate&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json | tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json
</span></span></code></pre></div><h3 id="evaluation">Evaluation<a hidden class="anchor" aria-hidden="true" href="#evaluation">#</a></h3>
<p>🥳 Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model&rsquo;s performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let&rsquo;s experiment with the following variables to find out.</p>
<h4 id="training-data-size">Training data size<a hidden class="anchor" aria-hidden="true" href="#training-data-size">#</a></h4>
<p>As training data increases, the model performs better on the same test dataset (size = X rows).</p>
<table>
<thead>
<tr>
<th>Sampling ratio</th>
<th>Training data size</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>50%</td>
<td>4265</td>
<td>50.00%</td>
</tr>
<tr>
<td>70%</td>
<td>5971</td>
<td>83.40%</td>
</tr>
<tr>
<td>90%</td>
<td>7677</td>
<td>88.74%</td>
</tr>
<tr>
<td>100%</td>
<td>8530</td>
<td>87.24%</td>
</tr>
</tbody>
</table>
<p><em>To be continued &hellip;</em></p>
<h2 id="how-is-the-new-way-different-from-the-traditional-way-of-adapting-ai">How is the new way different from the traditional way of adapting AI?<a hidden class="anchor" aria-hidden="true" href="#how-is-the-new-way-different-from-the-traditional-way-of-adapting-ai">#</a></h2>
<p>When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.</p>
<h4 id="the-traditional-way">The Traditional Way<a hidden class="anchor" aria-hidden="true" href="#the-traditional-way">#</a></h4>
<p>Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.</p>
<p>However, during my experience in the industry, I&rsquo;ve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intentions and insights are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p><img loading="lazy" src="images/model_vs_data_centric.png#center" alt="Model-Centric vs Data-Centric"  />
</p>
<div style="font-size: 14px; color: grey; text-align: center;">
  
The model-centric approach vs. data-centric approach (Image Source: generated by Midjourney)

</div>


<h4 id="the-new-way">The New Way<a hidden class="anchor" aria-hidden="true" href="#the-new-way">#</a></h4>
<p>In contrast, when adapting a finely-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.</p>
<p><img loading="lazy" src="images/new_vs_old_ai_dev.png#center" alt="old_way"  />

<div style="font-size: 14px; color: grey; text-align: center;">
  
The old way vs. new way of AI/ML development (Image Source: Snorkel AI)

</div>

</p>
<p><em>Parameter tuning is to be experimented&hellip;</em><br>
<em>Unsupervised learning is to be explored using LLM fine-tuing&hellip;</em></p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1]</p>
<h2 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h2>
<ul>
<li>Exploration of leveraging LLM to predict clusters</li>
<li>Investigate misclassified samples to gather insights about data collection &amp; quality</li>
<li>The impacts of parameters fine-tuning on model performance (e.g. learning rate)</li>
<li>Exploration of LoRa in excelerating fine-tuning</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://runrunicd.github.io/blog/tags/large-language-model/">large-language-model</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://runrunicd.github.io/blog/">Shel (Run) Zhou&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
