<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI/ML on Shel (Run) Zhou&#39;s Blog</title>
    <link>https://runrunicd.github.io/blog/categories/ai/ml/</link>
    <description>Recent content in AI/ML on Shel (Run) Zhou&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://runrunicd.github.io/blog/categories/ai/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Everything About RAG. Learn In Progress</title>
      <link>https://runrunicd.github.io/blog/posts/2024-01-22-rag/</link>
      <pubDate>Mon, 22 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://runrunicd.github.io/blog/posts/2024-01-22-rag/</guid>
      <description>Introduction Imagine you are Harry Potter. You want to find everything about the fantastic beast, Niffler, to prepare for your exam. You&amp;rsquo;re led to the library and wish to retrieve all relevant pieces of information from the books, for example, the beast likes shiny objects, is british, and chubby and cute, and visualize Niffler. Then, all you need is RAG :)
RAG stands for &amp;ldquo;Retrieval-Augmented Generation.&amp;rdquo; This approach can be particularly powerful for tasks like question answering, where it&amp;rsquo;s important to provide responses that are not only fluent and coherent (thanks to the generative model) but also deeply grounded in specific information retrieved from the text corpus.</description>
    </item>
    
    <item>
      <title>Embeddings Use Cases</title>
      <link>https://runrunicd.github.io/blog/posts/2023-11-14-embeddings/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://runrunicd.github.io/blog/posts/2023-11-14-embeddings/</guid>
      <description>Introduction When I first heard about text embeddings, I was perplexed. My tech lead handed me an 800-page Natural Language Processing textbook, but I did not finish reading it. Now, with the capabilities of OpenAI models, it&amp;rsquo;s time to learn and broadly leverage embeddings to accomplish many tasks that were hard be achieved by human labor or simple data analysis, thanks to easy access to these embeddings.
Let&amp;rsquo;s start exploring:</description>
    </item>
    
    <item>
      <title>Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Share Insight on Applications</title>
      <link>https://runrunicd.github.io/blog/posts/2023-08-28-llm-fine-tuning/</link>
      <pubDate>Mon, 28 Aug 2023 21:34:17 -0700</pubDate>
      
      <guid>https://runrunicd.github.io/blog/posts/2023-08-28-llm-fine-tuning/</guid>
      <description>Introduction I recently delved into two insightful books: &amp;ldquo;Free Your Mind&amp;rdquo; and &amp;ldquo;The Inner Game of Tennis&amp;rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&amp;rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.</description>
    </item>
    
    <item>
      <title>Learning Notes on Large Language Model (Dumping Knowledge for Now)</title>
      <link>https://runrunicd.github.io/blog/posts/2023-08-28-llm-notes/</link>
      <pubDate>Tue, 01 Aug 2023 13:33:00 -0700</pubDate>
      
      <guid>https://runrunicd.github.io/blog/posts/2023-08-28-llm-notes/</guid>
      <description>Messy &amp;hellip; Introduction to Large Language Model Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF).
Training methodology is simple but is limited to a few players with high computational requirements
Public pretrained LLM
BLOOM (Scao et al., 2022) LLaMa 1 (Touvron et al., 2023) LLaMa 2, LLaMa 2-Chat which is optimized for dialogue use cases Falcon (Penedo et al.</description>
    </item>
    
  </channel>
</rss>
