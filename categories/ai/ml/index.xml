<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI/ML on Shel (Run) Zhou&#39;s Blog</title>
    <link>https://runrunicd.github.io/blog/categories/ai/ml/</link>
    <description>Recent content in AI/ML on Shel (Run) Zhou&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://runrunicd.github.io/blog/categories/ai/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Build a Simple Framework for LLM Fine-Tuning on TPU &amp; Share Insight on Applications</title>
      <link>https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/</link>
      <pubDate>Mon, 28 Aug 2023 21:34:17 -0700</pubDate>
      
      <guid>https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/</guid>
      <description>Introduction I recently delved into two insightful books: &amp;ldquo;Free Your Mind&amp;rdquo; and &amp;ldquo;The Inner Game of Tennis&amp;rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It&amp;rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.
Take, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.</description>
    </item>
    
    <item>
      <title>Learning Notes on Large Language Model (Dumping Knowledge for Now)</title>
      <link>https://runrunicd.github.io/blog/post/2023-08-28-llm-reading-notes/</link>
      <pubDate>Tue, 01 Aug 2023 13:33:00 -0700</pubDate>
      
      <guid>https://runrunicd.github.io/blog/post/2023-08-28-llm-reading-notes/</guid>
      <description>Messy &amp;hellip; Introduction to Large Language Model Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF).
Training methodology is simple but is limited to a few players with high computational requirements
Public pretrained LLM
BLOOM (Scao et al., 2022) LLaMa 1 (Touvron et al., 2023) LLaMa 2, LLaMa 2-Chat which is optimized for dialogue use cases Falcon (Penedo et al.</description>
    </item>
    
  </channel>
</rss>
