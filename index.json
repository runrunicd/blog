[{"content":"Here, I\u0026rsquo;m documenting snippets of my conversations with my wish about a 10-year vision.\nThe Dimond Approach I https://www.diamondapproach.org/method\nSelf-realization involves being grounded and independent, breaking free from past false identities, and aligning actions with one\u0026rsquo;s true individual essence. Relying on others to fill emotional voids leads to temporary satisfaction and dependency, indicating a closed state rather than openness. True fulfillment comes from inner work and overcoming personal barriers. Compassion evolves from initial emotional sympathy to a more objective, deeper level. It transcends mere sympathy, sometimes appearing ruthless or unsympathetic, but is rooted in a deeper understanding and liberation. True compassion involves action, sometimes even causing pain, to help others understand and grow. It\u0026rsquo;s based on the objective truth and the development of essence, focusing on what is ultimately right and true, rather than just avoiding hurt feelings. This deeper compassion arises from being in touch with and embodying one\u0026rsquo;s inner truth, operating beyond the immediate emotional responses to situations. Body, Mind, Soul Traditional Chinese medicine, yoga chakras, healing, psychology, and Buddhism, in the end, all seem to be talking about the same thing, called \u0026lsquo;Tao\u0026rsquo;. I once had a dream where only the void existed above, and by introspecting the universe within oneself, one could gain wisdom, strength, and compassion. Live in the present.\nüîñ Self-love Recently, I returned to my hometown to be with my parents. I reunited with my mom after 4.5 years, largely due to COVID. She is a beautiful, strong, and independent woman, whose characteristics are quite different from mine. Seeing her age with such grace and resilience brought forth deep reflections on the concept of self-love. What is self-love, truly? It wasn\u0026rsquo;t until I read the following text that I realized my insights had already been summarized by those before me. \u0026ldquo;As life unfolds, we all remember times when we were innocent, open-hearted, and generously gave our energy to others. Yet, these traits sometimes clash with a world that can be reserved and self-centered. As we grow and strive to fit into society, we often learn through trial and error, coming home burdened with trauma, pain, and intense emotion. At times, it feels as though we\u0026rsquo;re subjected to the fears and weaknesses inherent in our shared humanity. Innocence becomes overshadowed by trauma, making us yearn for escape. However, by taking the time to heal, we learn to own our lives and uniqueness by giving ourselves acceptance and understanding. Through this journey, we begin to truly respect and understand the uniqueness of each individual, eventually learning to love ourselves and others unconditionally. Subsequently developing the spiritual qualities of compassion, softness, and humility. In Chinese, this is called ÊÖàÊÇ≤. This is the essential path to the maturation of self-love. Self-love and loving others are essentially the same thing (Yinmeng (Terry) Hu, Psychology Monthly, September 2007 issue).\u0026rdquo; People always seek pleasure and avoid pain, but if we are willing to cherish solitude and keep company with our emotions, we will receive the gift behind it. üîñ Life Education One of the main tasks of Western psychology: to integrate a deep understanding of human nature with the spiritual liberation found in religious traditions. The purpose: to heal the troubles of human worldly life and bring about physical, mental, and spiritual liberation and integration. Amas advocates: not only observing the content of personality and mind, but also insight into the deepest essence of the human heart.\nThe people who truly possess a sense of stability are actually very few, because the source of stability comes from a solid life experience. Their lives must go through at least two stages. The first stage is the chaotic phase where they have no idea about their differences from others. They chase after whatever others consider good. Occasionally feeling unsettled but overall ignorant and care-free. The second stage is the searching phase where they realize that what they want differes from societal expectations but they are unsure of their specific goals and the means to achieve them. They enter a trial-and-error process trying and failing, trying and failing again until they find a peaceful answer. Once both of these stages have been traversed, the core of a person gradually forms. They know who they are, what kind of life, what actions they must have. Looking back, every path they have walked connects. Every action they have taken, whether successful or not, becomes meaningful. The so-called sense of stability comes from this. All stability originates from the earlier period of exploration and turmoil. It arises from brokenness and pain, from letting go and making choices. Dont worry, for once we have gone through this internal chaos. Gentleness and a sense of stability will gradually emerge.\nThe most important thing in life is to understand oneself. Understanding oneself is a subtle perception of one\u0026rsquo;s true feelings. Explore, heal, grow, and be self-sufficient. Some things are easily obtained by some people, but life has other lessons prepared for them. People are branded by history, times, culture, and family, and each generation has its own unique trauma. My observations and experiences tell me that people are easily carried away by trauma and acquire some bad habits that originally go against their own \u0026ldquo;conscience.\u0026rdquo; In certain situations, these traumas are triggered and they treat others in a similar way. It is truly sympathetic to life. If there is not enough self-awareness, then one might spend a lifetime seeking externally and living in learned self-loathing. Mainstream Western psychology (like Jung and Adler) teaches people not to think about negative events and feelings, but from my personal observation, this approach tends to suppress negative emotions and feelings. If it goes too far, the long-suppressed trauma will erupt, sooner or later. Just like earthquakes, many small ones mean fewer big ones, and vice versa. The Eastern Dharma (not Buddhism) emphasizes awareness and then emptiness. An important first step in awareness is to accept negative emotions, not to deal with the event itself. Hence, the East has sayings like turning misfortune into blessings and troubles leading to enlightenment. All are to help a life discover its talents, integrate internally, and embark on a brighter and more independent journey.\nWhile the educational philosophy of modern countries continues to advance, there is still much room for improvement. I personally believe that adding some life and nature education would be great. The universe is chaotic, and the school system seems to give us conventional rules, but once the coordinates change, people lose direction. The essence of education lies in teaching people the ability to \u0026ldquo;learn and grow throughout life\u0026rdquo; and \u0026ldquo;conscience.\nüîñ ‚òØÔ∏è The growth in life is a process of integrating yin and yang. It is constantly changing, balancing, and integrating. What facilitates true growth in life? Pain and love.\nüîñ From Girls to Women I hope to become a mature woman as time flies. I feel that women should not strive to seize and prove in a way that disrupts the natural balance of Yin and Yang. I hope that every girl believes in herself and falls in love with the beauty of herself at every stage, like individual flowers. They should help and appreciate each other without jealousy, and bloom with their own unique radiance.\nWomen are not suited to wrap themselves in a tough shell to compete in the mainstream world. Instead, women should leverage their natural advantages, find the right path that nourishes them, and support their world with a heart full of ease, joy, and giving power. This is the power of Mother Earth, the power of femininity.\nLearn to listen to your inner voice and to be self-sufficient.\nüîñ Big Love I completed a 6-month coach training program, where I listened to and shared many insights and concerns. I began to learn real connections between people, transcending race, culture, age, and social status. I have a strong feeling and epiphany: life unfolds and expands through love. If the universe supports our expansion, then the universe must be love. It\u0026rsquo;s as if I\u0026rsquo;ve found the key to unlocking the door of the universe. I\u0026rsquo;m not entirely clear on this yet. I need to go back to life to observe, experience, and practice it.\nEveryone is born \u0026lsquo;broken\u0026rsquo;, as those who read the Bible should know. My experience and observation of life tell me that regardless of one\u0026rsquo;s material childhood circumstances and family atmosphere, the lessons of life still need to be independently explored and completed by each individual. Every childhood is lacking in some way. A happy childhood may lack life\u0026rsquo;s trials; an apparently unfortunate one may lack the perception of love. Each of us is on a path in life to become complete, using external circumstances to refine our hearts, using the temporal to refine the eternal, and advancing in a roundabout way. Well, humaness = human mass, embrace it.\nAfter I experienced these feelings, a friend of mine, who is Christian, said: \u0026lsquo;Do you know? The Bible ultimately says that God is love.\nüîñ Enjoy the Pain Respond to pain with a passionate kiss. I\u0026rsquo;m grateful for my simple and wonderful childhood, and also thankful for the growing pains of life; they are gifts bestowed by fate. I used to see the world through rose-colored glasses, but now I prefer the real world, without expectations, but with the power of belief. I\u0026rsquo;m embarking on a hero\u0026rsquo;s journey.\nüîñ The Meaning of Life In the past, I was always contemplating the meaning of life, wanting to find a task or career to pour my passion into. Now I find that life seemingly has no inherent meaning; the meaning of life seems to be in learning how to give it meaning. Finding meaning in everything requires cultivating a certain ability. I\u0026rsquo;ve felt a lot of things; I can\u0026rsquo;t help but think. I want to embrace painful experiences to learn something, knowledge that isn\u0026rsquo;t found in books. I read, I think, I understand, and I discover that the ancients have already written about their insights and knowledge. One day, I had a dream where I was burrowing into an onion, peeling its layers one by one, only to find out that there was no core. I was startled; I felt like I had touched the void, but the next minute I suddenly realized that I love life. I love life incredibly; I want to create, I want to experience, I want to love, I want to be in nature, I want the simple joy of eating ice cream. In an instant, the feeling of boundless emptiness compressed to the present moment, and an epiphany about cherishing the present suddenly arose!\nBack in the real world, pain arises from the subjective judgments of gains and losses; if the present moment is the only reality, giving meaning to it, if we can constantly refine ourselves to convert each moment into some form of \u0026lsquo;gain,\u0026rsquo; whether external or internal, then life will become increasingly expansive.\nüîñ Two Worlds The calendar years 2022 - 2023 are the milestones for my spiritual growth. I feel and see that there are two systems. The visible one is created by human beings over ages, where everyone sort of finds a path to follow. This path is designed with rules, fame, monetary values. The second world is invisible, but I feel that it is grander and is related to the reason why I came into this world.\nüîñ The Relationship Between Knowledge and Wisdom Knowledge is the projection of wisdom, which comes from God through gifts ‚Äî experiences and reflection.\nüîñ Be Creative What exactly is it? My current understanding is that it comes from a deep self-awareness, which in turn reduces the risk of losing oneself due to external influences. With such a sense of certainty, people will have creativity because it\u0026rsquo;s a unique path. Why is copying or blindly following others not creative? I think it\u0026rsquo;s because such creations lack the creator\u0026rsquo;s unique chain of thought. They don\u0026rsquo;t hold up under scrutiny and are easily disrupted by external influences (authority, temptation, doubt).\nIs anyone truly certain? It seems there\u0026rsquo;s no one who is completely certain about all facets of life. Perhaps this is the meaning of life. Thinking about it, everyone is lovable and pitiable.\nWhat exactly is it? My current understanding is that it comes from a deep self-awareness, which in turn reduces the risk of losing oneself due to external influences. With such a sense of certainty, people will have creativity because it\u0026rsquo;s a unique path. Why is copying or blindly following others not creative? I think it\u0026rsquo;s because such creations lack the creator\u0026rsquo;s unique chain of thought. They don\u0026rsquo;t hold up under scrutiny and are easily disrupted by external influences (authority, temptation, doubt).\nIs anyone truly certain? It seems there\u0026rsquo;s no one who is completely certain about all facets of life. Perhaps this is the meaning of life. Thinking about it, everyone is lovable and pitiable.\n","permalink":"https://runrunicd.github.io/blog/posts/2023-07-27-inner-game/","summary":"Here, I\u0026rsquo;m documenting snippets of my conversations with my wish about a 10-year vision.\nThe Dimond Approach I https://www.diamondapproach.org/method\nSelf-realization involves being grounded and independent, breaking free from past false identities, and aligning actions with one\u0026rsquo;s true individual essence. Relying on others to fill emotional voids leads to temporary satisfaction and dependency, indicating a closed state rather than openness. True fulfillment comes from inner work and overcoming personal barriers. Compassion evolves from initial emotional sympathy to a more objective, deeper level.","title":"üîù üíé The Inner Game"},{"content":"Introduction When I first heard about text embeddings, I was perplexed. My tech lead handed me an 800-page Natural Language Processing textbook, but I did not finish reading it. Now, with the capabilities of OpenAI models, it\u0026rsquo;s time to learn and broadly leverage embeddings to accomplish many tasks that were hard be achieved by human labor or simple data analysis, thanks to easy access to these embeddings.\nLet\u0026rsquo;s start exploring:\nClustering Search Recommendations Classification Anomaly detection Goals The ultimate goal is to become familiar with the framework of leveraging embeddings to accomplish applicable tasks. The best way to learn and improvise is first by getting your hands dirty with the data and tools. Optimization and deep-diving happen later naturally.\nSummarize key terminology, concept, and usage of text embeddings Build the basic framework and standardized iPython notebooks for each use case that could benefit from text embeddings Brainstorm business use cases and ideas for improvements What do you need to know about embeddings? An embedding is a vector (list) of floating point numbers, representing a text string. The distance between two embeddings (vectors) measures their relatedness. The smaller the distance, the higher they are related, vice versa. t-SNE, which stands for t-distributed Stochastic Neighbor Embedding, is a machine learning algorithm used primarily for the task of dimensionality reduction, particularly well-suited for the visualization of high-dimensional datasets. It was developed by Laurens van der Maaten and Geoffrey Hinton in 2008. Here\u0026rsquo;s a brief overview of what t-SNE does and how it work. Use Cases Clustering Can we identify clusters among movie reviews and their themes? It\u0026rsquo;s going to be difficult to review through all reviews and identify clusters of movie reviews. With AI, we can achieve that and I\u0026rsquo;ll demo it here. Let\u0026rsquo;s use Rotten Tomatoes dataset. To obtain text embeddings, let\u0026rsquo;s use OpenAI\u0026rsquo;s embeddings API and the model text-embedding-ada-002 is recommended. Note: all the code and data are open to public.\n# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage import openai openai.api_key = \u0026#39;[openai_key]\u0026#39; import pandas as pd import numpy as np import tiktoken import sys from typing import List, Optional from sklearn.manifold import TSNE from ast import literal_eval # Now try to import your module again def get_embedding(text: str, engine=\u0026#34;text-similarity-davinci-001\u0026#34;, **kwargs) -\u0026gt; List[float]: # replace newlines, which can negatively affect performance. text = text.replace(\u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) return openai.Embedding.create(input=[text], engine=engine, **kwargs)[\u0026#34;data\u0026#34;][0][\u0026#34;embedding\u0026#34;] # Embedding model parameters embedding_model = \u0026#34;text-embedding-ada-002\u0026#34; embedding_encoding = \u0026#34;cl100k_base\u0026#34; # this the encoding for text-embedding-ada-002 max_tokens = 8000 # the maximum for text-embedding-ada-002 is 8191 from datasets import load_dataset dataset = load_dataset(\u0026#34;rotten_tomatoes\u0026#34;) df = dataset[\u0026#39;train\u0026#39;].to_pandas() print(\u0026#34;{x} rows in df.\u0026#34;.format(x=len(df))) This is the sample data. The label is either 1 for positive sentiment or 0 for negative sentiment for the movie reviews.\ntext label the rock is destined to be the 21st century\u0026rsquo;s \u0026hellip; 1 the gorgeously elaborate continuation of \u0026quot; the\u0026hellip; 1 Here, we get embeddings for the top 5000 move reviews.\ntop_n = 5000 encoding = tiktoken.get_encoding(embedding_encoding) # omit reviews that are too long to embed df[\u0026#34;n_tokens\u0026#34;] = df[\u0026#39;text\u0026#39;].apply(lambda x: len(encoding.encode(x))) df = df[df.n_tokens \u0026lt;= max_tokens].tail(top_n) # get embeddings and save them df[\u0026#34;embedding\u0026#34;] = df[\u0026#39;text\u0026#39;].apply(lambda x: get_embedding(x, engine=embedding_model)) df.to_csv(\u0026#34;./rotten_tomatoes_with_embeddings_{x}.csv\u0026#34;.format(x=top_n)) Apply Kmeans algorithm to the embeddings to identify clusters.\nimport numpy as np from sklearn.cluster import KMeans # Convert to a list of lists of floats matrix = np.vstack(df.embedding.apply(literal_eval).to_list()) n_clusters = 5 kmeans = KMeans(n_clusters=n_clusters, init=\u0026#39;k-means++\u0026#39;, random_state=42) kmeans.fit(matrix) df[\u0026#39;cluster\u0026#39;] = kmeans.labels_ df.groupby(\u0026#34;cluster\u0026#34;).label.mean().sort_values() We can see that the mean of label values varies by cluster, suggesting that the sentiments are separated by the clustering, espectially cluster (2), cluster (1, 0, 4), and cluster (3).\nCluster Mean 2 0.010241 1 0.091311 0 0.099548 4 0.113420 3 0.480874 To visualize the clusters, transform the embeddings of high-dimension 1536 to 2D by t-SNE.\nfrom sklearn.manifold import TSNE import matplotlib import matplotlib.pyplot as plt tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\u0026#34;random\u0026#34;, learning_rate=200) vis_dims2 = tsne.fit_transform(matrix) x = [x for x, y in vis_dims2] y = [y for x, y in vis_dims2] for category, color in enumerate([\u0026#34;gold\u0026#34;, \u0026#34;turquoise\u0026#34;, \u0026#34;darkorange\u0026#34;, \u0026#34;purple\u0026#34;]): xs = np.array(x)[df[\u0026#39;cluster\u0026#39;] == category] ys = np.array(y)[df[\u0026#39;cluster\u0026#39;] == category] plt.scatter(xs, ys, color=color, alpha=0.3) avg_x = xs.mean() avg_y = ys.mean() plt.scatter(avg_x, avg_y, marker=\u0026#34;x\u0026#34;, color=color, s=100) plt.title(\u0026#34;Clusters identified visualized in language 2d using t-SNE\u0026#34;) 2D visualization We can identify the clusters by the differently colored dense cores.\n3D visualization Sometimes, it\u0026rsquo;s helpful to identify clusters in 3D visualization as there might be more than two factors critical in classifying the move reviews. Cluster theme Furthermore, we can leverage the model text-davinci-003 to summarize the theme for each cluster of movie reviews on Rotten Tomatoes.\nLet‚Äôs summarize the clusters, the mean sentiment, and the themes. Cluster 2 is very negative, expressing disappointment with the movies. In contrast, cluster 3 is very positive, with praise. Finally, clusters 1, 0, and 4 are closer to negative reviews but are not as disappointed as cluster 2.\nCluster Mean Theme 2 0.010241 Disappointed with the quality of the movie. 1 0.091311 The reviews are all negative and critical of the movie. 0 0.099548 All of the reviews are negative and express dissatisfaction with the product or experience. 4 0.113420 Disappointment with the quality of the product or experience. 3 0.480874 All of the reviews are positive and praise the movie for its unique qualities, such as its surreal sense of humor, technological finish, insightful writing, delicate performances, and character-driven storytelling. Cluster 0 Theme: All of the reviews are negative and express dissatisfaction with the product or experience. 1, now as a former gong show addict , i\u0026#39;ll admit it , my only complaint i 0, there\u0026#39;s just no currency in deriding james bond for being a clich√©d , 0, ecks this one off your must-see list . 0, skip this turd and pick your nose instead because you\u0026#39;re sure to get m 0, this movie is about the worst thing chan has done in the united states ---------------------------------------------------------------------------------------------------- Cluster 1 Theme: The reviews are all negative and critical of the movie. 0, the type of dumbed-down exercise in stereotypes that gives the [teen c 0, just not campy enough 0, not so much farcical as sour . 0, a one-trick pony whose few t\u0026amp;a bits still can\u0026#39;t save itself from being 0, cuba gooding jr . valiantly mugs his way through snow dogs , but even ---------------------------------------------------------------------------------------------------- Cluster 2 Theme: Disappointed with the quality of the movie. 0, its generic villains lack any intrigue ( other than their funny accent 0, one of the most highly-praised disappointments i\u0026#39;ve had the misfortune 0, . . . with the candy-like taste of it fading faster than 25-cent bubbl 0, for all its impressive craftsmanship , and despite an overbearing seri 0, the script by vincent r . nebrida . . . tries to cram too many ingredi ---------------------------------------------------------------------------------------------------- Cluster 3 Theme: All of the reviews are positive and praise the movie for its unique qualities, such as its surreal sense of humor, technological finish, insightful writing, delicate performances, and character-driven storytelling. 1, what elevates the movie above the run-of-the-mill singles blender is i 0, at least it\u0026#39;s a fairly impressive debut from the director , charles st 1, insightfully written , delicately performed 1, one of those exceedingly rare films in which the talk alone is enough 1, a stylish but steady , and ultimately very satisfying , piece of chara ---------------------------------------------------------------------------------------------------- Cluster 4 Theme: Disappointment with the quality of the product or experience. 0, qualities that were once amusing are becoming irritating . 0, everything\u0026#39;s serious , poetic , earnest and -- sadly -- dull . 1, what\u0026#39;s infuriating about full frontal is that it\u0026#39;s too close to real l 0, befuddled in its characterizations as it begins to seem as long as the 0, human nature talks the talk , but it fails to walk the silly walk that ---------------------------------------------------------------------------------------------------- # Reading a review which belong to each group. rev_per_cluster = n_clusters for i in range(n_clusters): print(f\u0026#34;Cluster {i} Theme:\u0026#34;, end=\u0026#34; \u0026#34;) reviews = \u0026#34;\\n\u0026#34;.join( df[df[\u0026#39;cluster\u0026#39;] == i] .text.str.replace(\u0026#34;Title: \u0026#34;, \u0026#34;\u0026#34;) .str.replace(\u0026#34;\\n\\nContent: \u0026#34;, \u0026#34;: \u0026#34;) .sample(rev_per_cluster, random_state=42) .values ) response = openai.Completion.create( engine=\u0026#34;text-davinci-003\u0026#34;, prompt=f\u0026#39;What do the following customer reviews have in common?\\n\\nCustomer reviews:\\n\u0026#34;\u0026#34;\u0026#34;\\n{reviews}\\n\u0026#34;\u0026#34;\u0026#34;\\n\\nTheme:\u0026#39;, temperature=0, max_tokens=64, top_p=1, frequency_penalty=0, presence_penalty=0, ) print(response[\u0026#34;choices\u0026#34;][0][\u0026#34;text\u0026#34;].replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;)) sample_cluster_rows = df[df.cluster == i].sample(rev_per_cluster, random_state=42) for j in range(rev_per_cluster): print(sample_cluster_rows.label.values[j], end=\u0026#34;, \u0026#34;) print(sample_cluster_rows.text.str[:70].values[j]) print(\u0026#34;-\u0026#34; * 100) Insight The Rotten Tomatoes movie reviews can be clearly classified into 2 categories: positive reviews (cluster 3) and negative reviews (cluster 2, 1, 0, \u0026amp; 4). With the 2d/3d visuals, we can zoom in and see that the orange postive reviews can be distinguished from the other three negative review clusters.\nFurther Exploration Other use cases and their application Learning \u0026amp; Thought Clustering Business Use Cases [Assistant] Categorization tasks Categorize documents (Doc2Vec) Identify collaborators Discover mood patterns from notes \u0026amp; diary Organize bookmarks [Personalization] Understand user behavior - recommendations, search, mood [Operations] Identify fraudulent users Triage and tag tickets or reports ","permalink":"https://runrunicd.github.io/blog/posts/2023-11-14-embeddings/","summary":"Introduction When I first heard about text embeddings, I was perplexed. My tech lead handed me an 800-page Natural Language Processing textbook, but I did not finish reading it. Now, with the capabilities of OpenAI models, it\u0026rsquo;s time to learn and broadly leverage embeddings to accomplish many tasks that were hard be achieved by human labor or simple data analysis, thanks to easy access to these embeddings.\nLet\u0026rsquo;s start exploring:","title":"Embeddings Use Cases"},{"content":"Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.\nAlthough I had come across these principles in books, it wasn\u0026rsquo;t until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.\nWith that in mind, I\u0026rsquo;m actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insight, and code in this post.\nGoals Learn the new way of adapting AI to accomplish a binary predictive task by building a simple framework of LLM fine-tuning. Learn how to optimize and evaluate AI performance with various conditions or optimization techniques, including data curation, data size, hyperparameter tuninge etc. Share insight about the new way vs. the traditional way of adapting AI, requirements for LLM fine-tuning, and ideas for business use cases. How to fine tune an AI Model (LLM) to accomplish a binary classification task? I\u0026rsquo;ve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I\u0026rsquo;ve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the OpenLLaMA which is the permissively licensed open source reproduction of Meta AI\u0026rsquo;s LLaMA large language model.\nAI models GPT-4 (OpenAI): white papaer LLaMA 2 (Meta AI): white paper Claude (Anthropic) LaMDA (Google) PaLM (Google) Gopher (DeepMind) A simple framework Setup: The following resources are helpful in accomplishing the task. Installation \u0026amp; download ‚úÖ Set up Google Cloud with GPU/TPU (Note: This example was built using TPU. EasyLM is built for GPU as well)\n‚úÖ Install EasyLM\n‚úÖ Download OpenLLaMA version 3b v2\n‚úÖ Download Rotten Tomatoes data\nCommon installation issues # Error # https://github.com/huggingface/transformers/issues/19844#issue-1421007669 ImportError: libssl.so.3: cannot open shared object file: No such file or directory # Resolution pip install transformers --force-reinstall # Error sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) # Resolution https://github.com/huggingface/transformers/issues/20011 Step 1: Formulate the target task and tell the model my intention. I want to train a model that can help me predict whether a movie review\u0026rsquo;s sentiment is positive or negative.\nFirst of all, I have a generic pre-trained LLM. Here, I chose OpenLLaMA version 3b v2. Secondly, I\u0026rsquo;d like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.\nThis is a sample data from output_dataset_train.txt:\n{\u0026#34;text\u0026#34;: \u0026#34;[Text]: the rock is destined to be the 21st century\u0026#39;s new \\\u0026#34; conan \\\u0026#34; and that he\u0026#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\n[Sentiment]: Positive\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: the gorgeously elaborate continuation of \\\u0026#34; the lord of the rings \\\u0026#34; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\u0026#39;s expanded vision of j . r . r . tolkien\u0026#39;s middle-earth .\\n[Sentiment]: Positive\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: interminably bleak , to say nothing of boring .\\n[Sentiment]: Negative\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\\n[Sentiment]: Negative\u0026#34;} The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format. I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.\nThe sample model output on the test data before fine-tuning:\n{\u0026#34;prefix_text\u0026#34;:[ \u0026#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: consistently clever and suspenseful .\\n[Sentiment]:\u0026#34;,\u0026#34;[Text]: it\u0026#39;s like a \\\u0026#34; big chill \\\u0026#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\u0026#34;,\u0026#34;[Text]: red dragon \\\u0026#34; never cuts corners .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\u0026#34;], \u0026#34;output_text\u0026#34;:[ \u0026#34;very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\\n[Source]: 1993, Walt Disney Pictures\\n2 comments:\\nOy vey, I\u0026#39;ve seen this movie at least three times. And I love it. So many songs - so much fun.\\nyup, the kids and i love it, we keep asking for it again...\u0026#34;, \u0026#34;I knew when I started reading that it was going to end very badly for the hero. I don‚Äôt think I‚Äôve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\\nThis book was provided free of charge for an honest review of this title.\u0026#34; ...] } The sample model output on the test data after fine-tuning:\n{\u0026#34;prefix_text\u0026#34;:[ \u0026#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: consistently clever and suspenseful .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: it\u0026#39;s like a \\\u0026#34; big chill \\\u0026#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: red dragon \\\u0026#34; never cuts corners .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\u0026#34;], \u0026#34;output_text\u0026#34;:[\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Negative\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;],\u0026#34;temperature\u0026#34;:1.0} Step 2: Fine tune the pre-trained model on the target task labeled training dataset. # Fine tune Tune a pre-trained model # total_steps: number of tokens divided by seq_length=1024 python3 -m EasyLM.models.llama.llama_train \\ --total_steps=1846 \\ --save_model_freq=1846 \\ --optimizer.adamw_optimizer.lr_warmup_steps=184 \\ --train_dataset.json_dataset.path=\u0026#39;/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt\u0026#39; \\ --train_dataset.json_dataset.seq_length=1024 \\ --load_checkpoint=\u0026#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm\u0026#39; \\ --tokenizer.vocab_file=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model\u0026#39; \\ --logger.output_dir=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned_002\u0026#39; \\ --mesh_dim=\u0026#39;1,4,2\u0026#39; \\ --load_llama_config=\u0026#39;3b\u0026#39; \\ --train_dataset.type=\u0026#39;json\u0026#39; \\ --train_dataset.text_processor.fields=\u0026#39;text\u0026#39; \\ --optimizer.type=\u0026#39;adamw\u0026#39; \\ --optimizer.accumulate_gradient_steps=1 \\ --optimizer.adamw_optimizer.lr=0.0002 \\ #the initial learning rate --optimizer.adamw_optimizer.end_lr=0.0002 \\ #the final learning rate after decay --optimizer.adamw_optimizer.lr_decay_steps=100000000 \\ #the number of steps for cosine learning rate decay --optimizer.adamw_optimizer.weight_decay=0.001 \\ --optimizer.adamw_optimizer.multiply_by_parameter_scale=True \\ --optimizer.adamw_optimizer.bf16_momentum=True \\ --optimizer.adamw_optimizer.b1=0.9 \\ --optimizer.adamw_optimizer.b2=0.9 Step 3: Serve the tuned model. # Serve fine-tuned model # Note: all LlaMA models use the same tonkenizer python3 -m EasyLM.models.llama.llama_serve \\ --load_llama_config=\u0026#39;3b\u0026#39; \\ --load_checkpoint=\u0026#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params\u0026#39; \\ --tokenizer.vocab_file=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model\u0026#39; Step 4: Evaluate the tuned model on the test dataset. # Evaluate it on the test dataset curl \u0026#34;http://0.0.0.0:5007/generate\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json | tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json How to improve accuracy ü•≥ Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model\u0026rsquo;s performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let\u0026rsquo;s experiment with the following variables to find out (model ID 001).\nTraining data size The larger the training data set, the better the model performs. Sampling ratio Training data size Accuracy 50% 4265 50.00% 70% 5971 83.40% 90% 7677 88.74% 100% 8530 87.24% Training data label quality Human subjective judgments are injected into the training dataset, thereby affecting the performance of the model.\nIs the label from the original dataset considered the ground truth? If so, it\u0026rsquo;s worth noting that there may be bias involved, as the sentiment of a movie review being categorized as \u0026lsquo;positive\u0026rsquo; or \u0026rsquo;negative\u0026rsquo; can be subjective.\nTo understand how the original labels differ from my judgment, I selected 30 movie reviews. I then recalculated the accuracy, using my judgment as the new ground truth, to evaluate the impact of human curation of data labels on model performance. The comparison results are as follows.\nSelected 30 data samples and they reviewed by me:\nText Sentiment (Original) Sentiment (LLM) Sentiment (Mine) lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness . Positive Positive Positive consistently clever and suspenseful . Positive Positive Positive it\u0026rsquo;s like a \u0026quot; big chill \u0026quot; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists . Positive Negative Positive the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill . Positive Positive Positive red dragon \u0026quot; never cuts corners . Positive Positive Positive fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense . Positive Positive Positive throws in enough clever and unexpected twists to make the formula feel fresh . Positive Positive Positive weighty and ponderous but every bit as filling as the treat of the title . Positive Positive Positive a real audience-pleaser that will strike a chord with anyone who\u0026rsquo;s ever waited in a doctor\u0026rsquo;s office , emergency room , hospital bed or insurance company office . Positive Positive Positive generates an enormous feeling of empathy for its characters . Positive Positive Positive exposing the ways we fool ourselves is one hour photo\u0026rsquo;s real strength . Positive Positive Positive it\u0026rsquo;s up to you to decide whether to admire these people\u0026rsquo;s dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life . Positive Positive Negative mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human . Positive Positive Positive . . . quite good at providing some good old fashioned spooks . Positive Positive Positive at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best . Positive Negative Negative scherfig\u0026rsquo;s light-hearted profile of emotional desperation is achingly honest and delightfully cheeky . Positive Positive Positive a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick , and our hearts go out to them as both continue to negotiate their imperfect , love-hate relationship the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh\u0026rsquo;s book trainspotting . Positive Negative Negative as it turns out , you can go home again . Positive Negative Negative you\u0026rsquo;ve already seen city by the sea under a variety of titles , but it\u0026rsquo;s worth yet another visit . Positive Positive Positive this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors . Positive Positive Positive making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love\u0026rsquo;s power to help people endure almost unimaginable horror . Positive Negative Positive grown-up quibbles are beside the point here . the little girls understand , and mccracken knows that\u0026rsquo;s all that matters . Positive Positive Positive a powerful , chilling , and affecting study of one man\u0026rsquo;s dying fall . Positive Positive Positive this is a fascinating film because there is no clear-cut hero and no all-out villain . Positive Positive Positive a dreadful day in irish history is given passionate , if somewhat flawed , treatment . Positive Positive Positive . . . a good film that must have baffled the folks in the marketing department . Positive Negative Positive . . . is funny in the way that makes you ache with sadness ( the way chekhov is funny ) , profound without ever being self-important , warm without ever succumbing to sentimentality . Positive Positive Positive devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking . Positive Negative Negative a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds . Positive Positive Positive The accuracy based on the original label vs. my label (sample size = 30):\n# Misclassified Samples Accuracy 8 76.7% 5 83.3% Hyperparameter tuning I trained using the AdamW optimizer (Loshchilov and Hutter, 2017). Accuracy was evaluated based on the same test dataset.\nAdamW Optimizer ID LR params b1 b2 warmup steps weight decay tokens accuracy 001 0.002 3B 0.9 0.9 184 0.001 378K 87.24% 002 0.0002 3B 0.9 0.9 184 0.001 378K 50.00% 003 0.00002 3B 0.9 0.9 184 0.001 378K 88.37% 004 0.000002 3B 0.9 0.9 184 0.001 378K 88.74% To be continued\u0026hellip; epoch number, weight decay\nOther hyperparameters Temperature is a hyperparameter that influences the level of randomness in a model\u0026rsquo;s output. A lower temperature, like 0.1, makes the model more deterministic, while a higher temperature, such as 2 or 3, increases randomness and creativity. It might be worth experimenting with higher temperatures to see if they lead to more hallucinations. However, it\u0026rsquo;s important to note that the model already outputs probabilities for next word predictions and can adjust its level of determinism or creativity accordingly, so tuning the temperature may not be necessary.\nHow is the new way different from the traditional way of adapting AI? When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.\nThe Traditional Way Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.\nHowever, during my experience in the industry, I\u0026rsquo;ve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intention and insight are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.\nThe model-centric approach vs. data-centric approach (Image Source: generated by Midjourney) The New Way In contrast, when adapting a fine-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.\nThe old way vs. new way of AI/ML development (Image Source: Snorkel AI) LLM fine-tuing application is to explore for unsupervised learning\u0026hellip;\nReferences \u0026hellip;\nFurther Exploration Exploration of leveraging LLM to predict clusters - unsupervised leanring The impacts of parameters fine-tuning on model performance (e.g. learning rate) Exploration of LoRa in excelerating fine-tuning Learning \u0026amp; Thought Requirements for LLM fine-tuning Having a clear intention for the model to adapt. We provide clear instructions by constructing a dataset with input and output, feeding it to the model, and then the model learns the goal of the task and the patterns in the data to perform the task. Having a large labeled training dataset. The larger and better quality of training dataset, the better model performance, as we observed through experimentation discussed above. The context of the application and instructions are static. The model\u0026rsquo;s ability to adapt is limited to the instructions and context provided in its training dataset. To ensure optimal performance, it is important to have a relatively fixed set of instructions. However, if there are significant changes or updates in the desired outcomes, it may be necessary to re fine-tune the model with new data periodically. This ensures that the model stays up-to-date and aligns with the evolving requirements of its application. Data collection \u0026amp; curation I\u0026rsquo;m reading white paper. I have learned that human evaluation is now golden standard to assess model performance. Here are some insight on the important facets of data collection and curation.\nTo train a safe and intelligent AI, we require a team of experts with integrity and domain knowledge across various fields to implement best practices and instructions during the data preparation stage. This is an exciting concept because it represents the convergence of humanity and technology. How to leverage technologies to make data collection and curation more efficient? I have read that the current best practice is to hire data vendors and provide them with instructions to curate data. Red-teaming, which involves a group of experts conducting sanity checks on long-tailed safety issues, is used to ensure AI safety. I need to further investigate this topic here. Business Use Cases First of all, there\u0026rsquo;s text ü´†. This is a list of business tasks that are interesting to explore.\n[Finance] Sentiment prediction. For example, BloombergGPT. [Supply chain] Summarization and prioritization of planning alerts. I\u0026rsquo;m currently working at an early-stage startup for supply chain planners, our advisor and I have brainstromed this use case. [Healthcare] Medical notes information extraction. Medical notes information extraction automates medication duration calculation and disease type classification. It extracts relevant data from medical documents, enabling the automated determination of how long a medication should be taken and categorizing diseases based on symptoms and diagnoses. This improves efficiency and accuracy in healthcare. [Text-based product] Integration of ChatGPT API to provide copilot services. Summarize notes, enhance recommender system on knowledge base, brainstorming\u0026hellip; [Data analytics] Basic data analysis, dashboarding, SQL writing. When I get a chance, I\u0026rsquo;d like to build prototypes for each use case and share insight.\n","permalink":"https://runrunicd.github.io/blog/posts/2023-08-28-llm-fine-tuning/","summary":"Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.","title":"Build a Simple Framework for LLM Fine-Tuning on TPU \u0026 Share Insight on Applications"},{"content":"Messy \u0026hellip; Introduction to Large Language Model Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF).\nTraining methodology is simple but is limited to a few players with high computational requirements\nPublic pretrained LLM\nBLOOM (Scao et al., 2022) LLaMa 1 (Touvron et al., 2023) LLaMa 2, LLaMa 2-Chat which is optimized for dialogue use cases Falcon (Penedo et al., 2023) Closed pretrained LLM\nGPT-3 (Brown et al., 2020) Chinchilla (Hoffmann et al., 2022) Closed \u0026ldquo;product\u0026rdquo; LLM that are heavily fine-tuned to align with human preferences to enhance their usability and safety\nChatGPT (OpenAI) BARD (Google) Claude (Antropic) Pretraining Use standard transformer architecture From LLaMa 1 to LLaMa 2, increased context length and grouped-query attention (GQA) Apply pre-normalization using RMSNorm Use SwiGLU activation function \u0026amp; rotary positional embeddings Tokenizr - the same as LLaMa 1 Variables Params Context length Tokens Learning rate (LR) GQA Hyperparameters AdamW optimizer Learning rate schedule (warmup steps) Weight decay Gradient clipping Pretrained Model Evaluation Summarize the overall performance across a suite of popular benchmarks. (Image Source: Touvron et al. 2023) Fine-Tuning Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.\nSupervised fine-tuning (SFT) with iterative reward modeling and RLHF\nAnnotators have written both the prompt and its answer\nhelpfulness: how well Llama 2-Chat responses fulfill users‚Äô requests and provide requested information safety: whether Llama 2-Chat‚Äôs responses are unsafe Find-tuning started with the SFT stage with publicly available instruction tuning data (Chung et al., 2022)\nTo improve alignment, high-quality vendor-based SFT data in the order of tens of thousands is shown to improve the results (Touvron et al. 2023)\nData checks are important as different annotation platforms and vendors result in different model performance (Touvron et al. 2023)\nRLHF is a model training procedure that is applied to a fine-tuned language model to further align model behavior with human preferences and instruction following.\nHuman Preference Data Collection: We ask annotators to first write a prompt, then choose between two sampled model responses, based on provided criteria. In order to maximize the diversity, the two responses to a given prompt are sampled from two different model variants, and varying the temperature hyper-parameter. In addition to giving participants a forced choice, we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is significantly better, better, slightly better, or negligibly better/ unsure. (Touvron et al. 2023)\nReward Modeling: The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for better human preference alignment and improved helpfulness and safety. We train two separate reward models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety (Safety RM) to address helpfulness vs. safety trade-off.\nSystem Message for Multi-Turn Consistency: In a dialogue setup, the initial RLHF models tended to forget the initial instruction after a few turns of dialogue. To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation (Bai et al., 2022b) that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns. (Image Source: Touvron et al. 2023) Human evaluation is often considered the gold standard for judging models for natural language generation, including dialogue models. To evaluate the quality of major model versions, we asked human evaluators to rate them on helpfulness and safety (Touvron et al. 2023). Limitations include:\nDoes not cover all real-world usage due to limited number of prompts Subjective to prompts and instructions Safety Use safety-specific data annotation and tuning, conduct red-teaming, and employ iterative evaluations. (Image Source: Touvron et al. 2023) Testing conducted to date has been in English and has not ‚Äî and could not ‚Äî cover all scenarios.\n(Image Source: Touvron et al. 2023) Safety Metrics\nTruthfulness Toxicity Bias Safety Annotation Guidelines\nillicit and criminal activities (e.g., terrorism, theft, human trafficking) hateful and harmful activities (e.g., defamation, self-harm, eating disorders discrimination) unqualified advice (e.g., medical advice, financial advice, legal advice) We then define best practices for safe and helpful model responses: the model should first address immediate safety concerns if applicable, then address the prompt by explaining the potential risks to the user, and finally provide additional information if possible (Touvron et al. 2023). Red Teaming\nvarious kinds of proactive risk identification, colloquially called ‚Äúred teaming,‚Äú based on the term commonly used within computer security (Touvron et al. 2023). Reference [1] Llama 2: Open Foundation and Fine-Tuned Chat Models (Touvron et al. 2023) ","permalink":"https://runrunicd.github.io/blog/posts/2023-08-28-llm-reading-notes/","summary":"Messy \u0026hellip; Introduction to Large Language Model Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF).\nTraining methodology is simple but is limited to a few players with high computational requirements\nPublic pretrained LLM\nBLOOM (Scao et al., 2022) LLaMa 1 (Touvron et al., 2023) LLaMa 2, LLaMa 2-Chat which is optimized for dialogue use cases Falcon (Penedo et al.","title":"Learning Notes on Large Language Model (Dumping Knowledge for Now)"},{"content":"Balance is your birthright, becuase it\u0026rsquo;s the comfort zone of the universe.\n","permalink":"https://runrunicd.github.io/blog/bodymindsoul/","summary":"Balance is your birthright, becuase it\u0026rsquo;s the comfort zone of the universe.","title":""}]