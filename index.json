[{"content":"Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur. As the product gains momentum in the market, it attracts both investment and talent who help refine and expand it. In the realm of sports, particularly tennis, top performers thrive by establishing practice systems. They relish the tactile sensation of hitting tennis balls, trust their own judgment while letting go of their ego, and commit to continuous learning and adaptation.\nAlthough I had come across these principles in books, it wasn\u0026rsquo;t until recently that I experienced the thrill of reconstructing, enhancing, and fine-tuning these systems to adapt to the present needs and conditions.\nWith that in mind, I\u0026rsquo;m actively applying this philosophy to my exploration of LLM Fine-Tuning. I will share my learning notes, insights, and code in this post.\nGoals Learn the new way of adapting AI to accomplish a binary classification task by building a simple framework of LLM fine-tuning. Learn how to optimize and evaluate AI performance with various conditions or optimization techniques (e.g. training data size). Share insights about the new way vs. the traditional way of adapting AI. How to fine tune an AI Model (LLM) to accomplish a binary classification task? I\u0026rsquo;ve had fun to generate content by ChatGPT from OpenAI in response to prompts, but how does one train (or fine-tune) a LLM to accomplish a target prediction task? I\u0026rsquo;ve tailored a task to predict the sentiments of movie reviews from Rotten Tomatoes, using the OpenLLaMA which is the permissively licensed open source reproduction of Meta AI\u0026rsquo;s LLaMA large language model.\nAI models\nGPT-4 (OpenAI): white papaer LLaMA 2 (Meta AI): white paper Claude (Anthropic) LaMDA (Google) PaLM (Google) Gopher (DeepMind) A simple framework Setup: The following resources are helpful in accomplishing the task. Installation \u0026amp; download: ✅ Set up Google Cloud with GPU/TPU (Note: I have TPU. EasyLM is built for GPU as well)\n✅ Install EasyLM\n✅ Download OpenLLaMA version 3b v2\n✅ Download Rotten Tomatoes data\nCommon installation issues: # Error # https://github.com/huggingface/transformers/issues/19844#issue-1421007669 ImportError: libssl.so.3: cannot open shared object file: No such file or directory # Resolution pip install transformers --force-reinstall # Error sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) # Resolution https://github.com/huggingface/transformers/issues/20011 Step 1: Formulate the target task and tell the model my intention. I want to train a model that can help me predict whether a movie review\u0026rsquo;s sentiment is positive or negative.\nFirst of all, I have a generic pre-trained LLM. Here, I chose OpenLLaMA version 3b v2. Secondly, I\u0026rsquo;d like to tell the model my intention by preparing a dataset of movie reviews and labeled sentiments.\nThis is a sample data from output_dataset_train.txt:\n{\u0026#34;text\u0026#34;: \u0026#34;[Text]: the rock is destined to be the 21st century\u0026#39;s new \\\u0026#34; conan \\\u0026#34; and that he\u0026#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\n[Sentiment]: Positive\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: the gorgeously elaborate continuation of \\\u0026#34; the lord of the rings \\\u0026#34; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\u0026#39;s expanded vision of j . r . r . tolkien\u0026#39;s middle-earth .\\n[Sentiment]: Positive\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: interminably bleak , to say nothing of boring .\\n[Sentiment]: Negative\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;[Text]: things really get weird , though not particularly scary : the movie is all portent and no content .\\n[Sentiment]: Negative\u0026#34;} The Rotten Tomatoes dataset directly downloaded from HuggingFace is in json format. The Python script prepare_dataset_json2txt.py can convert data in json to text format. I did a little experiment to test if the model has learned the goal of the task. Compare the evaluation output before vs. after training (fine-tuning), one can see that the fine-tuned model has learned my intention that its job is to predict positive or negative sentiment of a movie review.\nThe sample model output on the test data before fine-tuning:\n{\u0026#34;prefix_text\u0026#34;:[ \u0026#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: consistently clever and suspenseful .\\n[Sentiment]:\u0026#34;,\u0026#34;[Text]: it\u0026#39;s like a \\\u0026#34; big chill \\\u0026#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\u0026#34;,\u0026#34;[Text]: red dragon \\\u0026#34; never cuts corners .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\u0026#34;], \u0026#34;output_text\u0026#34;:[ \u0026#34;very charming and entertaining, the whole family will enjoy the magic, the singing, the singing, the talking, the talking, the singing and the talking\\n[Source]: 1993, Walt Disney Pictures\\n2 comments:\\nOy vey, I\u0026#39;ve seen this movie at least three times. And I love it. So many songs - so much fun.\\nyup, the kids and i love it, we keep asking for it again...\u0026#34;, \u0026#34;I knew when I started reading that it was going to end very badly for the hero. I don’t think I’ve ever seen a more beautifully crafted book by a new-to-me author. It is a little dark, but so beautiful and the writing will definitely linger in my mind long after I have closed the book.\\n2 stars (out of 3); 2014 reading challenge: 2 for 3 (book #2 of 12 fiction)\\nThis book was provided free of charge for an honest review of this title.\u0026#34; ...] } The sample model output on the test data after fine-tuning:\n{\u0026#34;prefix_text\u0026#34;:[ \u0026#34;[Text]: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: consistently clever and suspenseful .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: it\u0026#39;s like a \\\u0026#34; big chill \\\u0026#34; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: red dragon \\\u0026#34; never cuts corners .\\n[Sentiment]:\u0026#34;, \u0026#34;[Text]: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\\n[Sentiment]:\u0026#34;], \u0026#34;output_text\u0026#34;:[\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Negative\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;,\u0026#34;Positive\u0026#34;],\u0026#34;temperature\u0026#34;:1.0} Step 2: Fine tune the pre-trained model on the target task labeled training dataset. # Fine tune Tune a pre-trained model # total_steps: number of tokens divided by seq_length=1024 python3 -m EasyLM.models.llama.llama_train \\ --total_steps=1846 \\ --save_model_freq=1846 \\ --optimizer.adamw_optimizer.lr_warmup_steps=184 \\ --train_dataset.json_dataset.path=\u0026#39;/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/output_dataset_train.txt\u0026#39; \\ --train_dataset.json_dataset.seq_length=1024 \\ --load_checkpoint=\u0026#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/open_llama_3b_v2_easylm\u0026#39; \\ --tokenizer.vocab_file=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model\u0026#39; \\ --logger.output_dir=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned\u0026#39; \\ --mesh_dim=\u0026#39;1,4,2\u0026#39; \\ --load_llama_config=\u0026#39;3b\u0026#39; \\ --train_dataset.type=\u0026#39;json\u0026#39; \\ --train_dataset.text_processor.fields=\u0026#39;text\u0026#39; \\ --optimizer.type=\u0026#39;adamw\u0026#39; \\ --optimizer.accumulate_gradient_steps=1 \\ --optimizer.adamw_optimizer.lr=0.002 \\ --optimizer.adamw_optimizer.end_lr=0.002 \\ --optimizer.adamw_optimizer.lr_decay_steps=100000000 \\ --optimizer.adamw_optimizer.weight_decay=0.001 \\ --optimizer.adamw_optimizer.multiply_by_parameter_scale=True \\ --optimizer.adamw_optimizer.bf16_momentum=True Step 3: Serve the tuned model. # Serve fine-tuned model # Note: all LlaMA models use the same tonkenizer python3 -m EasyLM.models.llama.llama_serve \\ --load_llama_config=\u0026#39;3b\u0026#39; \\ --load_checkpoint=\u0026#39;params::/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm_tuned/6680d4286a394c999852dcfe33081c44/streaming_params\u0026#39; \\ --tokenizer.vocab_file=\u0026#39;/checkpoint/xinleic/tune/EasyLM/my_models/open_llama_3b_v2_easylm/tokenizer.model\u0026#39; Step 4: Evaluate the tuned model on the test dataset. # Evaluate it on the test dataset curl \u0026#34;http://0.0.0.0:5007/generate\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST --data-binary @/checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test.json | tee /checkpoint/xinleic/tune/EasyLM/data/rotten_tomatoes/eval_output_dataset_test_a4tune.json How to improve accuracy 🥳 Now, I have built a simple framework to train and evaluate a Language Learning Model (LLM). I am curious about which variables impact the model\u0026rsquo;s performance. In practice, a finely-tuned model with a high level of accuracy is essential for handling business-specific tasks. Let\u0026rsquo;s experiment with the following variables to find out.\nTraining data size The larger the training data set, the better the model performs. Sampling ratio Training data size Accuracy 50% 4265 50.00% 70% 5971 83.40% 90% 7677 88.74% 100% 8530 87.24% Training data label quality Human subjective judgments are injected into the training dataset, thereby affecting the performance of the model.\nIs the label from the original dataset considered the ground truth? If so, it\u0026rsquo;s worth noting that there may be bias involved, as the sentiment of a movie review being categorized as \u0026lsquo;positive\u0026rsquo; or \u0026rsquo;negative\u0026rsquo; can be subjective.\nTo understand how the original labels differ from my judgment, I selected 30 movie reviews. I then recalculated the accuracy, using my judgment as the new ground truth, to evaluate the impact of human curation of data labels on model performance. The comparison results are as follows.\nSelected 30 data samples and they reviewed by me:\nText Sentiment (Original) Sentiment (LLM) Sentiment (Mine) lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness . Positive Positive Positive consistently clever and suspenseful . Positive Positive Positive it\u0026rsquo;s like a \u0026quot; big chill \u0026quot; reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists . Positive Negative Positive the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill . Positive Positive Positive red dragon \u0026quot; never cuts corners . Positive Positive Positive fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense . Positive Positive Positive throws in enough clever and unexpected twists to make the formula feel fresh . Positive Positive Positive weighty and ponderous but every bit as filling as the treat of the title . Positive Positive Positive a real audience-pleaser that will strike a chord with anyone who\u0026rsquo;s ever waited in a doctor\u0026rsquo;s office , emergency room , hospital bed or insurance company office . Positive Positive Positive generates an enormous feeling of empathy for its characters . Positive Positive Positive exposing the ways we fool ourselves is one hour photo\u0026rsquo;s real strength . Positive Positive Positive it\u0026rsquo;s up to you to decide whether to admire these people\u0026rsquo;s dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life . Positive Positive Negative mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human . Positive Positive Positive . . . quite good at providing some good old fashioned spooks . Positive Positive Positive at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best . Positive Negative Negative scherfig\u0026rsquo;s light-hearted profile of emotional desperation is achingly honest and delightfully cheeky . Positive Positive Positive a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick , and our hearts go out to them as both continue to negotiate their imperfect , love-hate relationship the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh\u0026rsquo;s book trainspotting . Positive Negative Negative as it turns out , you can go home again . Positive Negative Negative you\u0026rsquo;ve already seen city by the sea under a variety of titles , but it\u0026rsquo;s worth yet another visit . Positive Positive Positive this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors . Positive Positive Positive making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love\u0026rsquo;s power to help people endure almost unimaginable horror . Positive Negative Positive grown-up quibbles are beside the point here . the little girls understand , and mccracken knows that\u0026rsquo;s all that matters . Positive Positive Positive a powerful , chilling , and affecting study of one man\u0026rsquo;s dying fall . Positive Positive Positive this is a fascinating film because there is no clear-cut hero and no all-out villain . Positive Positive Positive a dreadful day in irish history is given passionate , if somewhat flawed , treatment . Positive Positive Positive . . . a good film that must have baffled the folks in the marketing department . Positive Negative Positive . . . is funny in the way that makes you ache with sadness ( the way chekhov is funny ) , profound without ever being self-important , warm without ever succumbing to sentimentality . Positive Positive Positive devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking . Positive Negative Negative a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds . Positive Positive Positive The accuracy based on the original label vs. my label (sample size = 30):\n# Misclassified Samples Accuracy 8 76.7% 5 83.3% To be continued \u0026hellip;\nHyperparameter tuning How is the new way different from the traditional way of adapting AI? When building the framework and fine-tuning the LLM, I began to think how this approach differs from the traditional method of developing a binary classifier in ML. Here is my take based on my past industry experience builing Machine Learning models.\nThe Traditional Way Traditionally, data scientists are trained in academic settings to focus on algorithms. These algorithms serve as specialized tools in a toolkit, each designed to solve specific problems. For instance, supervised machine learning algorithms are employed to discern patterns and make predictions when ground truth labels are available. Commonly used algorithms include logistic regression for predicting binary events and XGBoost for class prediction, where trade-offs between precision and recall are managed through threshold selections. In the realm of unsupervised machine learning, where the task is to identify clusters without ground truth, classical algorithms like k-means, hierarchical clustering, DBSCAN, and Gaussian Mixture Models are often used.\nHowever, during my experience in the industry, I\u0026rsquo;ve discovered that data quality and feature engineering are equally crucial for adapting a machine learning model to achieve high accuracy in business tasks. Why is this the case? My intuition suggests that human intentions and insights are encapsulated within these two components. Data quality ensures that clear ground truth labels are provided to the model, while feature engineering infuses business logic or common sense into the model by clarifying and filtering out noise.\nThe model-centric approach vs. data-centric approach (Image Source: generated by Midjourney) The New Way In contrast, when adapting a finely-tuned Large Language Model (LLM) to perform a similar predictive task, the architecture of the model is essentially fixed. The pre-trained model already has many capabilities. How, then, do I guide the model to accomplish a specific task? Mostly, this is done through careful data preparation. This data encapsulates both my intentions and any available ground truth, particularly in the context of supervised learning.\nThe old way vs. new way of AI/ML development (Image Source: Snorkel AI) LLM fine-tuing application is to explore for unsupervised learning\u0026hellip;* References \u0026hellip;\nFurther Exploration Exploration of leveraging LLM to predict clusters - unsupervised leanring The impacts of parameters fine-tuning on model performance (e.g. learning rate) Exploration of LoRa in excelerating fine-tuning Thoughts Data collection \u0026amp; curation Application ","permalink":"https://runrunicd.github.io/blog/post/2023-08-28-llm-fine-tuning/","summary":"Introduction I recently delved into two insightful books: \u0026ldquo;Free Your Mind\u0026rdquo; and \u0026ldquo;The Inner Game of Tennis\u0026rdquo;. Both have shed light on the concept of building systems that encourage growth and resilience. It\u0026rsquo;s fascinating how everything starts with a simple framework ignited by a spark; it then gains momentum and expands through garnering support, optimization, and evolution.\nTake, for example, the process of scaling a business from the ground up. It often starts with a minimal viable product (MVP) built by an entrepreneur.","title":"Build a Simple Framework for LLM Fine-Tuning on TPU \u0026 Applications"},{"content":"There are inspirations in my head from time to time. I\u0026rsquo;m documenting them here as fuel for my side project of writing a book.\n🔖 Two Worlds The calendar years 2022 - 2023 are the milestones for my spiritual growth. I feel and see that there are two systems. The visible one is created by human beings over ages, where everyone sort of finds a path to follow. This path is designed with rules, fame, monetary values. The second world is invisible, but I feel that it is grander and is related to the reason why I came into this world.\n🔖 The Meaning of Life In the past, I was always contemplating the meaning of life, wanting to find a task or career to pour my passion into. Now I find that life seemingly has no inherent meaning; the meaning of life seems to be in learning how to give it meaning. Finding meaning in everything requires cultivating a certain ability. I\u0026rsquo;ve felt a lot of things; I can\u0026rsquo;t help but think. I want to embrace painful experiences to learn something, knowledge that isn\u0026rsquo;t found in books. I read, I think, I understand, and I discover that the ancients have already written about their insights and knowledge. One day, I had a dream where I was burrowing into an onion, peeling its layers one by one, only to find out that there was no core. I was startled; I felt like I had touched the void, but the next minute I suddenly realized that I love life. I love life incredibly; I want to create, I want to experience, I want to love, I want to be in nature, I want the simple joy of eating ice cream. In an instant, the feeling of boundless emptiness compressed to the present moment, and an epiphany about cherishing the present suddenly arose!\nBack in the real world, pain arises from the subjective judgments of gains and losses; if the present moment is the only reality, giving meaning to it, if we can constantly refine ourselves to convert each moment into some form of \u0026lsquo;gain,\u0026rsquo; whether external or internal, then life will become increasingly expansive.\n🔖 The Relationship Between Knowledge and Wisdom Knowledge is the projection of wisdom, which comes from God through gifts—experiences and reflection.\n🔖 Big Love I completed a 6-month coach training program, where I listened to and shared many insights and concerns. I began to experience real love and the connections between people, transcending race, culture, age, and social status. I have a strong feeling and epiphany: life unfolds and expands through love. If the universe supports our expansion, then the universe must be love. It\u0026rsquo;s as if I\u0026rsquo;ve found the key to unlocking the door of the universe. I\u0026rsquo;m not entirely clear on this yet. I need to go back to life to observe, experience, and practice it.\n🔖 Enjoy The Pain Respond to pain with a passionate kiss. I\u0026rsquo;m grateful for my simple and wonderful childhood, and also thankful for the growing pains of life; they are gifts bestowed by fate.\n🔖 Be Creative Everyone is born \u0026lsquo;broken,\u0026rsquo; as those who read the Bible should know. My experience and observation of life tell me that regardless of one\u0026rsquo;s material childhood circumstances and family atmosphere, the lessons of life still need to be independently explored and completed by each individual. Every childhood is lacking in some way. A happy childhood may lack life\u0026rsquo;s trials; an apparently unfortunate one may lack the perception of love. Each of us is on a path in life to become complete, using external circumstances to refine our hearts, using the temporal to refine the eternal, and advancing in a roundabout way.\nReturning to creativity, what exactly is it? On the surface, it seems to be something completely new or a combination of many points. My current understanding is that it comes from a deep self-awareness, which in turn reduces the risk of losing oneself due to external influences. With such a sense of certainty, people will have creativity because it\u0026rsquo;s a unique path. Why is copying or blindly following others not creative? I think it\u0026rsquo;s because such creations lack the creator\u0026rsquo;s unique chain of thought. They don\u0026rsquo;t hold up under scrutiny and are easily disrupted by external influences (authority, temptation, doubt).\nIs anyone truly certain? It seems there\u0026rsquo;s no one who is completely certain about all facets of life. Perhaps this is the meaning of life. Thinking about it, everyone is lovable and pitiable.\nWell, humaness = human mass, embrace it.\n","permalink":"https://runrunicd.github.io/blog/post/2023-07-27-inner-game/","summary":"There are inspirations in my head from time to time. I\u0026rsquo;m documenting them here as fuel for my side project of writing a book.\n🔖 Two Worlds The calendar years 2022 - 2023 are the milestones for my spiritual growth. I feel and see that there are two systems. The visible one is created by human beings over ages, where everyone sort of finds a path to follow. This path is designed with rules, fame, monetary values.","title":"The Inner Game"}]