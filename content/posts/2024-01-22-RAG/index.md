---
title: "Everything about RAG"
date: 2024-01-22T00:00:00
draft: false
tags: ["RAG"]
categories: ["AI/ML"]
---

## Introduction
Imagine you are Harry Potter. You want to find everything about the fantastic beast, Niffler, to prepare for your exam. You're led to the library and wish to retrieve all relevant pieces of information from the books and visualize Niffler, for example, the beast likes shiny objects, is british, and chubby and cute. Then, all you need is RAG :)

RAG stands for "Retrieval-Augmented Generation." This approach can be particularly powerful for tasks like question answering, where it's important to provide responses that are not only fluent and coherent (thanks to the generative model) but also deeply grounded in specific information retrieved from the text corpus. The idea behind RAG is to combine the strengths of two types of models: a retrieval model and a generative model.

**Retrieval Model - Finding the Puzzle Pieces:** In our analogy, the retrieval model is like having a magic bookshelf. Whenever you need a specific puzzle piece, you describe what you're looking for, and the bookshelf presents you with a selection of puzzle pieces that closely match your description. These pieces are like the relevant pieces of information or text snippets retrieved from a vast database. The bookshelf doesn't give you the exact position where each piece goes, but it ensures the pieces are relevant to the part of the puzzle you're working on.

**Generative Model - Putting the Puzzle Together:** Once you have the right pieces (information), the generative model is like your skill in putting these pieces together in a way that makes sense. It looks at the pieces you've gathered, considers how they fit with what you've already assembled (the context or the prompt you're responding to), and then skillfully places them to form a coherent picture (or, in the case of RAG, a coherent piece of text).



![harry_potter](images/harry_potter.png#center)
{{< embedded_citation >}}
Harry Potter hopes to utilize RAG to collect clues on niffler. (image generated by Midjourney)
{{< /embedded_citation >}}
![niffler](images/niffler.png#center)
{{< embedded_citation >}}
Niffler visualized through clues. (image generated by Midjourney)
{{< /embedded_citation >}}


## Visualization of RAG System
Here's an animated overview of a RAG system, including:
- Processing documents: 
  - load documents
  - create document chunks
  - vectorize document chunks -> embeddings + document chunk ID
  - store embeddings in vector database
- Processing user query: 
  - vectorize user query -> embedding
  - retrieve relevant document chunk IDs
  - retrieve document chunks from vector database by document chunk IDs
  - use LLM on user query + relevant document chunks to generate the response

![rag](images/rag_gif.gif#center)
{{< embedded_citation >}} Source: https://huggingface.co/blog/ray-rag {{< /embedded_citation >}}

## Challenges
- Bad retrieval
  - Low precision: not all chunks retrieved are relevant due to hallucination + lost in the middle problems.
  - Low recall: not all relevant chunks are retrieved due to limited context for LLM synthesize an answer.
  - Obsolete information: the data is out of date or redundant.

- Bad response
  - Hallucination: LLM makes up an answer that isn’t in the context.
  - Irrelevance: LLM makes up an answer that doesn’t answer the question.
  - Toxicity/bias: LLM makes up an answer that’s harmful or offensive.


## Improvements
- Data: Can we store additional information beyond raw text chunks?
- Embeddings: Can we optimize our embedding representations?
- Retrieval: Can we do better than top-k embedding lookup?
- Synthesis: can we use LLMs for more than generation?

## Evaluation
- How do we properly evaluate a RAG system?
  - Evaluate E23
    - Evaluate the final generated response given input
    - Create dataset 
      - Input: query
      - [Optional]output: the “ground-truth” answer
    - Run through full RAG pipeline
    - Collect evaluation metrics
      - If no labels: label-free evaluator
        - Faithfulness
        - Relevancy
        - Adheres to guidelines
        - Toxicity-free
      - If labels: with-label evaluator
        - Accuracy

  - Evaluate the separate parts (retrieval, synthesis). Diagnosis which part needs improvements? 
    - Retrieval - retriever evaluator (MRR, precision@k, NDCG)
      - Evaluate the quality of retrieved chunks given user query
      - Create dataset (human-labeled or synthetic)
        - Input: query
        - Output: the “ground-truth” documents relevant to the query
      - Run retriever over dataset
      - Measure ranking metrics
        - Success rate / hit-rate
        - MRR
        - Hit-rate

## Tools
- Langchain
- LlamaIndex

## Projects
WIP, stay tuned.